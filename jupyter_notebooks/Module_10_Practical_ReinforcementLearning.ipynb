{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\u26a0\ufe0f **Static Version Notice**\n",
    "\n",
    "This is a static export of an interactive marimo notebook. Some features have been modified for compatibility:\n",
    "\n",
    "- Interactive UI elements (sliders, dropdowns, text inputs) have been removed\n",
    "- UI variable references have been replaced with default values\n",
    "- Some cells may have been simplified or removed entirely\n",
    "\n",
    "For the full interactive experience, please run the original marimo notebook (.py file) using:\n",
    "```bash\n",
    "uv run marimo edit notebook_name.py\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 10: Practical - Basics of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "In the year 2147, Earth\u2019s surface has become a patchwork of unstable terrains after decades of climate decay.\n",
    "Our AI-controlled exploration unit \u2014 Bot-7 \u2014 is dispatched from its landing pod (\ud83d\udd35) to reach a high-priority extraction point (\ud83c\udfc1).\n",
    "\n",
    "The environment is treacherous and energy-limited. Bot-7 must autonomously learn the best route across a grid of dynamic terrain:\n",
    "\n",
    "\ud83c\udf31 Biofields: Soft, moss-covered terrain. Easy to traverse.\n",
    "\n",
    "\ud83c\udf0a Flooded Zones: Shallow but energy-draining water channels.\n",
    "\n",
    "\u26f0\ufe0f Crater Ridges: Volcanic rubble requiring intense power to cross.\n",
    "\n",
    "Each grid cell represents one unit of terrain. Moving across terrain consumes energy \u2014 some more than others.\n",
    "\n",
    "*Source: ChatGPT 4o (prompt: shortest path algorithm story with a sci-fi theme)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class GridGame:\n",
    "    TERRAIN_TYPES = {\n",
    "        \"\ud83c\udf31\": 1,  # Land\n",
    "        \"\ud83c\udf0a\": 3,  # Sea\n",
    "        \"\u26f0\ufe0f\": 5   # Mountain\n",
    "    }\n",
    "\n",
    "    def __init__(self, rows, cols, terrain_weights=None):\n",
    "        \"\"\"\n",
    "        :param rows: Number of rows in the grid\n",
    "        :param cols: Number of columns in the grid\n",
    "        :param terrain_weights: Optional weights for each terrain type\n",
    "        \"\"\"\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.terrain_symbols = list(self.TERRAIN_TYPES.keys())\n",
    "        self.terrain_costs = self.TERRAIN_TYPES\n",
    "\n",
    "        self.grid = self.generate_random_grid(rows, cols, terrain_weights)\n",
    "\n",
    "        self.cost_input_grid = {\n",
    "    (i,j): mo.ui.number(value = None) for i in range(rows) for j in range(cols) }\n",
    "\n",
    "        self.value_input_grid = {\n",
    "    (i,j): mo.ui.number(value = None) for i in range(rows) for j in range(cols) }\n",
    "\n",
    "        self.start, self.goal = self.pick_start_and_goal()\n",
    "        self.trajectory = []\n",
    "\n",
    "    def generate_random_grid(self, rows, cols, weights=None):\n",
    "        \"\"\"Generates a random terrain grid.\"\"\"\n",
    "        if weights is None:\n",
    "            weights = [1] * len(self.terrain_symbols)  # uniform if not provided\n",
    "        return [\n",
    "            [random.choices(self.terrain_symbols, weights=weights, k=1)[0] for _ in range(cols)]\n",
    "            for _ in range(rows)\n",
    "        ]\n",
    "\n",
    "    def pick_start_and_goal(self):\n",
    "        \"\"\"Randomly selects non-overlapping start and goal cells.\"\"\"\n",
    "        all_cells = [(i, j) for i in range(self.rows) for j in range(self.cols)]\n",
    "        start = random.choice(all_cells)\n",
    "        all_cells.remove(start)\n",
    "        goal = random.choice(all_cells)\n",
    "        return start, goal\n",
    "\n",
    "    def in_bounds(self, coord):\n",
    "        x, y = coord\n",
    "        return 0 <= x < self.rows and 0 <= y < self.cols\n",
    "\n",
    "    def get_cost(self, coord):\n",
    "        if not self.in_bounds(coord):\n",
    "            raise ValueError(f\"Coordinate {coord} is out of bounds.\")\n",
    "        terrain = self.grid[coord[0]][coord[1]]\n",
    "        return self.terrain_costs.get(terrain, float(\"inf\"))\n",
    "\n",
    "    def evaluate_trajectory(self, trajectory):\n",
    "        total_cost = 0\n",
    "        for coord in trajectory:\n",
    "            total_cost += self.get_cost(coord)\n",
    "            if coord == self.goal:\n",
    "                return total_cost, True\n",
    "        return total_cost, False\n",
    "\n",
    "    def evaluate_policy(self, policy, steps):\n",
    "        if not self.start:\n",
    "            raise ValueError(\"Start state must be defined.\")\n",
    "\n",
    "        actions = {\n",
    "            \"up\":    (-1, 0),\n",
    "            \"down\":  (1, 0),\n",
    "            \"left\":  (0, -1),\n",
    "            \"right\": (0, 1)\n",
    "        }\n",
    "\n",
    "        trajectory = []\n",
    "        current = self.start\n",
    "        total_cost = self.get_cost(current)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            if current == self.goal:\n",
    "                return total_cost, True\n",
    "\n",
    "            action = policy(current)\n",
    "            move = actions.get(action)\n",
    "            if move is None:\n",
    "                raise ValueError(f\"Invalid action '{action}' returned by policy.\")\n",
    "            next_state = (current[0] + move[0], current[1] + move[1])\n",
    "            if not self.in_bounds(next_state):\n",
    "                break\n",
    "            current = next_state\n",
    "            trajectory.append(current)\n",
    "            total_cost += self.get_cost(current)\n",
    "\n",
    "        return trajectory, total_cost, current == self.goal\n",
    "\n",
    "    def print_values(self, values, input_grid, test=False):\n",
    "        def color_style(value, color):\n",
    "            return f\"<span style='background-color:{color}; color:white; padding:2px 6px'>{value}</span>\"\n",
    "\n",
    "        def color_cell(index, value):\n",
    "            if (self.goal and index == self.goal):\n",
    "                return color_style(\"\ud83c\udfc1\", \"black\")\n",
    "            if (self.start and index == self.start):\n",
    "                return color_style(\"\ud83d\udd35\", \"blue\")  \n",
    "            if test:\n",
    "                return color_style(value, \"white\") + f\"{input_grid[index]}\"\n",
    "            else:\n",
    "                if input_grid[index].value == None:\n",
    "                    return \"\"\n",
    "                if input_grid[index].value == values[index]:\n",
    "                    return color_style( input_grid[index].value, \"green\")\n",
    "                else:\n",
    "                    return color_style( input_grid[index].value, \"red\")                \n",
    "        # Format rows\n",
    "        rows = []\n",
    "        for i, row in enumerate(self.grid):\n",
    "            formatted_row = [color_cell((i, j), val) for j, val in enumerate(row)]\n",
    "            rows.append(\"| \" + \" | \".join(formatted_row) + \" |\")\n",
    "\n",
    "        # Markdown table header separator\n",
    "        header_separator = \"| \" + \" | \".join([\"---\"] * len(self.grid[0])) + \" |\"\n",
    "        header = \"| \" + \" | \".join([\" \" for i in range(len(self.grid[0]))]) + \" |\"\n",
    "        # Insert header separator after first row\n",
    "        return \"\\n\".join([header, header_separator] + rows)\n",
    "\n",
    "    def print_grid(self, trajectory = None):\n",
    "        def color_style(value, color):\n",
    "            return f\"<span style='background-color:{color}; color:white; padding:2px 6px'>{value}</span>\"\n",
    "\n",
    "        def color_cell(index, value):\n",
    "            if (self.goal and index == self.goal):\n",
    "                return color_style(\"\ud83c\udfc1\", \"black\")\n",
    "            if (self.start and index == self.start):\n",
    "                return color_style(\"\ud83d\udd35\", \"blue\")  \n",
    "            if (trajectory and (index in trajectory)):\n",
    "                return color_style(value, \"green\")\n",
    "            else:\n",
    "                return color_style(value, \"white\")\n",
    "\n",
    "        # Format rows\n",
    "        rows = []\n",
    "        for i, row in enumerate(self.grid):\n",
    "            formatted_row = [color_cell((i, j), val) for j, val in enumerate(row)]\n",
    "            rows.append(\"| \" + \" | \".join(formatted_row) + \" |\")\n",
    "\n",
    "        # Markdown table header separator\n",
    "        header_separator = \"| \" + \" | \".join([\"---\"] * len(self.grid[0])) + \" |\"\n",
    "        header = \"| \" + \" | \".join([\" \" for i in range(len(self.grid[0]))]) + \" |\"\n",
    "        # Insert header separator after first row\n",
    "        return \"\\n\".join([header, header_separator] + rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "If the terrain map is known, Bot-7 can compute the most energy-efficient route to the target using principles of **dynamic programming** \u2014 evaluating each cell's cumulative cost from goal to start and choosing the least expensive path. In particular, let's assign the following costs to going through each type of terrain: \"\ud83c\udf31\": 1, \"\ud83c\udf0a\": 3, \"\u26f0\ufe0f\": 5. Below is a solution to this problem using a very well known algorithm known as Dijkstra's Algorithm (feel free to look at the code if you are interested, but in reinforcement learning our goal will be to understand how to do this without knowing the map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "# Note this is an artificial example that calculates the exact costs and rewards to be used in the remainder of the notebook for demonstration purposes. It combines both cost based calculations and reward value function calculation instead of relying on only one of the approaches.\n",
    "\n",
    "def dijkstra(grid, terrain_costs, start, goal):\n",
    "    \"\"\"\n",
    "    Finds the lowest-cost path using Dijkstra's algorithm.\n",
    "\n",
    "    :param grid: 2D list of terrain symbols (e.g. [[\"\ud83c\udf31\", \"\ud83c\udf0a\", \"\u26f0\ufe0f\"]])\n",
    "    :param terrain_costs: dict mapping symbols to cost (e.g. {\"\ud83c\udf31\": 1, \"\ud83c\udf0a\": 3, \"\u26f0\ufe0f\": 5})\n",
    "    :param start: (row, col) tuple\n",
    "    :param goal: (row, col) tuple\n",
    "    :return: (total_cost, path as list of (row, col))\n",
    "    \"\"\"\n",
    "    rows, cols = len(grid), len(grid[0])\n",
    "\n",
    "    def in_bounds(coord):\n",
    "        x, y = coord\n",
    "        return 0 <= x < rows and 0 <= y < cols\n",
    "\n",
    "    def get_cost(coord):\n",
    "        x, y = coord\n",
    "        return terrain_costs.get(grid[x][y], float(\"inf\"))\n",
    "\n",
    "    def get_neighbors(coord):\n",
    "        x, y = coord\n",
    "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        return [(x + dx, y + dy) for dx, dy in directions if in_bounds((x + dx, y + dy))]\n",
    "\n",
    "    visited = set()\n",
    "    heap = [(0, goal, [goal])]\n",
    "\n",
    "    costs = { (i, j): float('inf') for i in range(rows) for j in range(cols) }\n",
    "    costs[goal] = 0\n",
    "\n",
    "    path_to_goal = []\n",
    "    cost_to_goal = float(\"inf\")\n",
    "\n",
    "    while heap:\n",
    "        cost, current, path = heapq.heappop(heap)\n",
    "        if current in visited:\n",
    "            continue\n",
    "        visited.add(current)\n",
    "\n",
    "        if current == start:\n",
    "            path_to_goal = path\n",
    "            cost_to_goal = cost\n",
    "\n",
    "        for neighbor in get_neighbors(current):\n",
    "            new_cost = cost + get_cost(neighbor)\n",
    "            if new_cost < costs[neighbor]:\n",
    "                costs[neighbor] = new_cost\n",
    "                heapq.heappush(heap, (new_cost, neighbor, [neighbor] + path))\n",
    "\n",
    "    return cost_to_goal, path_to_goal, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = GridGame(rows=5, cols=5)\n",
    "\n",
    "cost, trajectory, costs = dijkstra(game.grid, game.terrain_costs, game.start, game.goal)\n",
    "print(f\"\\nTotal Cost: {cost}\")\n",
    "print(f\"Trajectory: {trajectory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"{game.print_grid()}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"{game.print_grid(trajectory)}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Dijkstra's algorithm (and more generally dynamic programming algorithms) rely on a simple intution, which will also be very useful to us later with RL.  From each cell in the grid there is a smallest cost path to the goal (not necessarily unique). The expected cost of this path is sometimes referred to as the **cost-to-go** of that state. Let's try to understand how to calculate the value function. Fill in the table below with what you think the cost of the optimal path will be (smallest total cost to the goal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"{game.print_values(costs, game.cost_input_grid, test=True)}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"{button.value}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Before we proceed further, we\u2019ll make one important conceptual shift. From this point on, we will refer not to costs, but to rewards.\n",
    "\n",
    "\n",
    "- In reinforcement learning, agents are trained to maximize reward rather than minimize cost.\n",
    "\n",
    "- Mathematically, the two are equivalent: *reward* = \u2212*cost*\n",
    "\n",
    "So when Bot-7 is faced with a grid of terrain, each movement will now yield a (negative) reward, representing energy loss. Its mission becomes one of **maximizing total reward**, which naturally leads to minimizing total energy use.\n",
    "\n",
    "This reward-based framing aligns with how most RL algorithms are formulated, and sets the stage for what comes next: value functions, policies, and learning through experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's now frame the problem as an RL problem and understand better the terms: **agent**, **environment**, **state**, **action**, **reward** as applied to this example.\n",
    "\n",
    "The **agent** is simply our Bot-7 (or its decision making system). Its **state** is its position in our grid matrix and can be described with coordinates (i,j) representing the row and column of the position. The **actions** available to Bot-7 are moving up, down, left, or right (except when it is at the boundary, then only some of the actions are available).\n",
    "\n",
    "The **environment** abstracts all the complexities of putting the agent in the next state based on its action decision. This could be the robot actuators, the effects of wind and anything else.  For this example we'll assume a deterministic environment that simply maps the current state and action to the next state:\n",
    "\n",
    "$$s(t+1) = f(s(t), a(t))$$\n",
    "\n",
    "by moving the robot to up, down, left, or right based on the action $a(t)$.\n",
    "\n",
    "Note: to make things more interesting we could introduce a stochastic environment that, for example, moves the bot to the state corresponding to the action with 80% probability, and moves it to one of the other neighboring states with a 20% probability.\n",
    "\n",
    "Finally, along with updating the state, the environment also assigns some **reward** (in our case corresponding to one of the terrain types the bot ended up in or an extra reward if it reached the goal)\n",
    "\n",
    "Rewards:\n",
    "\n",
    "- \ud83c\udf31: -1\n",
    "- \ud83c\udf0a: -3\n",
    "- \u26f0\ufe0f: -5\n",
    "\n",
    "Recall that in RL the agent usually does not have access to the map and doesn't know in advance which cells will give which rewards. It will have to discover it by trying various actions in an efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "For now, let's pretend again that we know what the map looks like:\n",
    "\n",
    "{game.print_grid()}\n",
    "\n",
    "The agent needs some way to make decisions based on the information it has. This is called a **policy**. Let's assume the agent's policy is to first move vertically to get to the same column as the goal and then move horizontally. Let's evaluate this policy to calculate the total amount of reward the agent would get.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple policy returning the direction to move \n",
    "# horizontally (+-1, 0) or vertically (0, +-1)\n",
    "def simple_policy(state):\n",
    "    x, y = state\n",
    "    gx, gy = game.goal\n",
    "    if (x < gx):\n",
    "        return (1, 0)\n",
    "    if (x > gx):\n",
    "        return (-1, 0)\n",
    "    if (y < gy):\n",
    "        return (0, 1)\n",
    "    if (y > gy):\n",
    "        return (0, -1)\n",
    "\n",
    "# Function to get the reward of moving to a new cell\n",
    "# This is part of the environment and is used only\n",
    "# to evaluate the policy, not to learn it.\n",
    "def get_reward(x, y):\n",
    "    return -game.terrain_costs.get(game.grid[x][y])\n",
    "\n",
    "# Function to take a step in the environment\n",
    "# This simulates the environment's response to the agent's action\n",
    "# and returns the new state and the reward\n",
    "def step(state, action):\n",
    "    x, y = state\n",
    "    dx, dy = action\n",
    "    nx, ny = x + dx, y + dy\n",
    "    return (nx, ny), get_reward(nx, ny)\n",
    "\n",
    "# Function to evaluate the policy by simulating the agent's actions\n",
    "# based on the policy we define, in this case: simple_policy\n",
    "def evaluate_policy():\n",
    "    state = game.start\n",
    "    trajectory = []\n",
    "    total_reward = 0\n",
    "    while state != game.goal:\n",
    "        action = simple_policy(state)\n",
    "        state, reward = step(state, action)\n",
    "        if state != game.goal:\n",
    "            total_reward += reward\n",
    "            trajectory.append(state)\n",
    "    print(f\"\\nTotal reward: {total_reward}\")\n",
    "    print(f\"Trajectory: {trajectory}\")\n",
    "    return trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"{game.print_grid(policy_eval_button.value)}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "As we see, following different policies results in the different rewards at the end of the  **episode** (note how we use the terms rollout, trajectory, episode interchangeably). The goal in RL is to find optimal policies resulting in largest possible reward for the agent.\n",
    "\n",
    "While learning the algorithms for finding such policies is outside the scope of this course, most popular methods rely on using some function approximators (like neural networks) to either learn the **value** function or the policy directly.\n",
    "\n",
    "Value functions are analogous to the optimal cost to the goal that we saw in the beginning of this notebook. Instead of giving the cost of the optimal path from each state, value functions give the total reward (or expected reward in the stochastic case) that the agent would gain by either following a specific policy (policy value function) or by following the optimal policy. To get more intution behind value functions we'll fill in the optimal value table analogous to the cost to the goal table.\n",
    "\n",
    "As you fill in this table refer to the slides and make sure you understand how these optimal values satisfy the Bellman Equation for the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "rewards = {}\n",
    "for key in costs:\n",
    "    rewards[key] = - costs[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"{game.print_values(rewards, game.value_input_grid, test=True)}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"{values_button.value}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "In control problems, where the goal is to come up with an optimal policy, a more useful function is the **q-function**, which is analogous to the value function, but keeps track of the total expected reward based on not only the current state, but also the current action. Some of the RL algorithms use function approximators like neural networks to **learn** the value/q function (remember that in real scenario you don't have a map so cannot do the calculation above) by trying or observing many episodes or alternating policy improvements and q-function approximations under current policy.\n",
    "\n",
    "Once you know the optimal value/q functions figuring out a policy is straight-forward, as the agent takes the action with the maximum expected reward (given by value/q functions). This is referred to as 'acting greedily' with respect to it.\n",
    "\n",
    "Another common approach is to forego the intermediate step of learning the value function and instead learn directly the policy mapping $a(t) = \\pi(s(t))$ from states to actions (this is particularly useful in the stochastic case when a neural network with a final softmax activation layer can give the probabilities of taking each action $a_i$ based on the current state $s(t)$,\n",
    "\n",
    "$$\n",
    "\\pi(a_i(t) | s(t)).\n",
    "$$\n",
    "\n",
    "We finish with an example of one such method. Understanding the details of this and other methods is outside the scope of this course, but feel free to explore and play around with it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient(grid, terrain_costs, start, goal, episodes=1000, gamma=0.99, lr=0.001):\n",
    "    rows, cols = len(grid), len(grid[0])\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "    action_to_delta = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "    n_actions = len(actions)\n",
    "\n",
    "    # Initialize policy weights \u03b8[state][action]\n",
    "    theta = np.zeros((rows, cols, n_actions))\n",
    "\n",
    "    def in_bounds(x, y):\n",
    "        return 0 <= x < rows and 0 <= y < cols\n",
    "\n",
    "    def get_reward(x, y):\n",
    "        return -terrain_costs.get(grid[x][y], -float('inf'))\n",
    "\n",
    "    def softmax(logits):\n",
    "        exps = np.exp(logits - np.max(logits))  # for stability\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "    def choose_action(state):\n",
    "        x, y = state\n",
    "        probs = softmax(theta[x][y])\n",
    "        action_index = np.random.choice(range(n_actions), p=probs)\n",
    "        return actions[action_index], action_index, probs\n",
    "\n",
    "    def step(state, action_name):\n",
    "        dx, dy = action_to_delta[action_name]\n",
    "        x, y = state\n",
    "        nx, ny = x + dx, y + dy\n",
    "        if in_bounds(nx, ny):\n",
    "            return (nx, ny), get_reward(nx, ny)\n",
    "        return (x, y), get_reward(x, y)  # no-op with penalty\n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(episodes):\n",
    "        state = start\n",
    "        trajectory = []\n",
    "        for _ in range(100):  # max steps per episode\n",
    "            action_name, action_idx, probs = choose_action(state)\n",
    "            next_state, reward = step(state, action_name)\n",
    "            trajectory.append((state, action_idx, reward))\n",
    "            state = next_state\n",
    "            if state == goal:\n",
    "                break\n",
    "\n",
    "        # Compute returns and update \u03b8\n",
    "        G = 0\n",
    "        for t in reversed(range(len(trajectory))):\n",
    "            state, action_idx, reward = trajectory[t]\n",
    "            G = reward + gamma * G\n",
    "            x, y = state\n",
    "            probs = softmax(theta[x][y])\n",
    "            grad = -probs\n",
    "            grad[action_idx] += 1  # \u2207log \u03c0\n",
    "            theta[x][y] += lr * G * grad  # policy gradient step\n",
    "\n",
    "    # Generate best path using learned policy\n",
    "    path = [start]\n",
    "    state = start\n",
    "    visited = set()\n",
    "    for _ in range(100):\n",
    "        x, y = state\n",
    "        if state == goal or state in visited:\n",
    "            break\n",
    "        visited.add(state)\n",
    "        probs = softmax(theta[x][y])\n",
    "        action_name = actions[np.argmax(probs)]\n",
    "        state, _ = step(state, action_name)\n",
    "        path.append(state)\n",
    "\n",
    "    total_cost = sum(get_reward(x, y) for x, y in path[1:-1])\n",
    "    return total_cost, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cost, p_trajectory = policy_gradient(game.grid, game.terrain_costs, game.start, game.goal, 10000)\n",
    "print(f\"\\nPolicy Gradient Cost: {p_cost}\")\n",
    "print(f\"Path: {p_trajectory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"{game.print_grid(p_trajectory)}\"\"\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}