{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 4: Practical 3 - Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "In this practical, we \"upgrade\" our Fully Connected Neural Network to a Convolutional Neural Network (CNN). Except for the model definition, most of the code here is identical to Practical 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check which GPU is available\n",
    "device = (\n",
    "    torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Prepare the Data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_img, first_label = train_dataset[0]\n",
    "print(\"Label: \", first_label)\n",
    "plt.imshow(first_img.permute(1, 2, 0).squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = np.array(\n",
    "    [\n",
    "        \"airplane\",\n",
    "        \"automobile\",\n",
    "        \"bird\",\n",
    "        \"cat\",\n",
    "        \"deer\",\n",
    "        \"dog\",\n",
    "        \"frog\",\n",
    "        \"horse\",\n",
    "        \"ship\",\n",
    "        \"truck\",\n",
    "    ]\n",
    ")\n",
    "CLASSES[first_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some individual pixel: \", train_dataset[54][0][1, 12, 13])\n",
    "print(\"Corresponding Label: \", train_dataset[54][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "_random_index = np.random.randint(len(train_dataset))\n",
    "_img, _label = train_dataset[_random_index]\n",
    "print(\"Label: \", _label, CLASSES[_label])\n",
    "plt.imshow(_img.permute(1, 2, 0).squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of the train dataset: \", len(train_dataset))\n",
    "print(\"Length of list(train_loader): \", len(list(train_loader)))\n",
    "print(\n",
    "    \"Shape of the first element of list(train_loader)[0]: \",\n",
    "    list(train_loader)[0][0].shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Dataloaders** provide multiple ways to access the data, either by converting it into a **Python list** or by using an **iterable**.\n",
    "\n",
    "Using `list(train_loader)`, as we have shown in Practical 1, loads the **entire dataset into memory**, which can be **slow** and even **fail** when dealing with large datasets.\n",
    "\n",
    "Since **neural network training algorithms process data in batches**, it is more efficient to use an **iterator**. Instead of retrieving the first batch like this:\n",
    "```python\n",
    "list(train_loader)[0]\n",
    "```\n",
    "which loads everything into memory, we use:\n",
    "```python\n",
    "next(iter(train_loader))\n",
    "```\n",
    "This approach retrieves only the first batch without loading the entire dataset, making it memory-efficient and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's load the first batch of our data (image and label) and display it using the `matplotlib` library.\n",
    "\n",
    "Recall that the shape returned by\n",
    "```python\n",
    "next(iter(train_loader))\n",
    "```\n",
    "is 32 by 3 by 32 by 32. This shape represents the batch size, number of channels, height, and width of the image, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_batch_images, next_batch_labels = next(iter(train_loader))\n",
    "_first_img = next_batch_images[0]  # retrieve the first image from the batch of 32\n",
    "_first_label = next_batch_labels[0]  # retrieve the first label from the batch of 32\n",
    "plt.imshow(\n",
    "    _first_img.permute(1, 2, 0)\n",
    ")  # imshow requires the image to be in height x width x channels format\n",
    "plt.show()\n",
    "print(\"Label: \", CLASSES[_first_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We now define our first CNN architecture. We will use this building block throughout the semester, so make sure you understand how it works. In particular, if we use the same padding and kernel (filter) size values for for both the width and height dimensions, the formula for the image size after the convolution or a max pooling layer is given by:\n",
    "\n",
    "$$\n",
    "\\frac{width + 2 * padding - (kernelsize -1) - 1}{S} + 1\n",
    "$$\n",
    "\n",
    "Verify this with `torch.nn.Conv2d` and `torch.nn.MaxPool2d` function documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, 16, kernel_size=3, padding=1\n",
    "        )  # Input channels = 3, Output channels = 16\n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # Pooling layer, will half the dimensions\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            16, 32, kernel_size=3, padding=1\n",
    "        )  # Input channels = 16, Output channels = 32\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output layer for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "input_shape_ui = mo.ui.array(\n",
    "    [mo.ui.number(value=3 if i == 0 else 32) for i in range(3)]\n",
    ")\n",
    "\n",
    "layer_0_shape_ui = mo.ui.array([mo.ui.number(value=None) for i in range(3)])\n",
    "layer_1_shape_ui = mo.ui.array([mo.ui.number(value=None) for i in range(3)])\n",
    "layer_2_shape_ui = mo.ui.array([mo.ui.number(value=None) for i in range(3)])\n",
    "layer_3_shape_ui = mo.ui.array([mo.ui.number(value=None) for i in range(3)])\n",
    "layer_4_shape_ui = mo.ui.array([mo.ui.number(value=None) for i in range(1)])\n",
    "layer_5_shape_ui = mo.ui.array([mo.ui.number(value=None) for i in range(1)])\n",
    "\n",
    "mo.md(\n",
    "    rf\"\"\"\n",
    "    ## Fill in the table based on the input shape\n",
    "\n",
    "    Input shape:\n",
    "\n",
    "    $B \\times$ {input_shape_ui[0]} $\\times$ {input_shape_ui[1]} $\\times$ {input_shape_ui[2]}\n",
    "\n",
    "    After the first convolution ($B \\times C \\times H \\times W$): \n",
    "\n",
    "    $B \\times$ {layer_0_shape_ui[0]} $\\times$ {layer_0_shape_ui[1]} $\\times$ {layer_0_shape_ui[2]}\n",
    "\n",
    "\n",
    "    After the first pooling ($B \\times C \\times H \\times W$): \n",
    "\n",
    "    $B \\times$ {layer_1_shape_ui[0]} $\\times$ {layer_1_shape_ui[1]} $\\times$ {layer_1_shape_ui[2]}\n",
    "\n",
    "    After the second convolution ($B \\times C \\times H \\times W$): \n",
    "\n",
    "    $B \\times$ {layer_2_shape_ui[0]} $\\times$ {layer_2_shape_ui[1]} $\\times$ {layer_2_shape_ui[2]}\n",
    "\n",
    "    After the second pooling ($B \\times C \\times H \\times W$): \n",
    "\n",
    "    $B \\times$ {layer_3_shape_ui[0]} $\\times$ {layer_3_shape_ui[1]} $\\times$ {layer_3_shape_ui[2]}\n",
    "\n",
    "    After the first fully connected layer ($B \\times C$): \n",
    "\n",
    "    $B \\times$ {layer_4_shape_ui[0]}\n",
    "\n",
    "    After the second fully connected layer ($B \\times C$): \n",
    "\n",
    "    $B \\times$ {layer_5_shape_ui[0]}\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "_C = input_shape_ui[0].value\n",
    "_H = input_shape_ui[1].value\n",
    "_W = input_shape_ui[2].value\n",
    "\n",
    "input_shape = [_C, _H, _W]\n",
    "layer_shape = []\n",
    "\n",
    "_Ht = (\n",
    "    _H + 2 * net.conv1.padding[0] - (net.conv1.kernel_size[0] - 1) - 1\n",
    ") / net.conv1.stride[0] + 1\n",
    "_W = (\n",
    "    _W + 2 * net.conv1.padding[1] - (net.conv1.kernel_size[1] - 1) - 1\n",
    ") / net.conv1.stride[1] + 1\n",
    "_H = _Ht\n",
    "_C = net.conv1.out_channels\n",
    "layer_shape.append([_C, _H, _W])\n",
    "\n",
    "_Ht = (\n",
    "    _H + 2 * net.pool.padding - (net.pool.kernel_size - 1) - 1\n",
    ") / net.pool.stride + 1\n",
    "_W = (\n",
    "    _W + 2 * net.pool.padding - (net.pool.kernel_size - 1) - 1\n",
    ") / net.pool.stride + 1\n",
    "_H = _W\n",
    "layer_shape.append([_C, _H, _W])\n",
    "\n",
    "_Ht = (\n",
    "    _H + 2 * net.conv2.padding[0] - (net.conv2.kernel_size[0] - 1) - 1\n",
    ") / net.conv2.stride[0] + 1\n",
    "_W = (\n",
    "    _W + 2 * net.conv2.padding[1] - (net.conv2.kernel_size[1] - 1) - 1\n",
    ") / net.conv2.stride[1] + 1\n",
    "_H = _Ht\n",
    "_C = net.conv2.out_channels\n",
    "layer_shape.append([_C, _H, _W])\n",
    "\n",
    "_Ht = (\n",
    "    _H + 2 * net.pool.padding - (net.pool.kernel_size - 1) - 1\n",
    ") / net.pool.stride + 1\n",
    "_W = (\n",
    "    _W + 2 * net.pool.padding - (net.pool.kernel_size - 1) - 1\n",
    ") / net.pool.stride + 1\n",
    "_H = _W\n",
    "layer_shape.append([_C, _H, _W])\n",
    "\n",
    "layer_shape.append([net.fc1.out_features])\n",
    "layer_shape.append([net.fc2.out_features])\n",
    "\n",
    "mo.md(\n",
    "    rf\"\"\"\n",
    "    After the first convolution layer $B \\times$ {layer_0_shape_ui[0].value} $\\times$ {layer_0_shape_ui[1].value} $\\times$ {layer_0_shape_ui[2].value}\n",
    "    {'✅' if (not any([layer_shape[0][i] != layer_0_shape_ui[i].value for i in range(3)])) else '❌'}\n",
    "\n",
    "    After the first pooling layer $B \\times$ {layer_1_shape_ui[0].value} $\\times$ {layer_1_shape_ui[1].value} $\\times$ {layer_1_shape_ui[2].value}\n",
    "    {'✅' if (not any([layer_shape[1][i] != layer_1_shape_ui[i].value for i in range(3)])) else '❌'}   \n",
    "\n",
    "    After the second convolution layer $B \\times$ {layer_2_shape_ui[0].value} $\\times$ {layer_2_shape_ui[1].value} $\\times$ {layer_2_shape_ui[2].value}\n",
    "    {'✅' if (not any([layer_shape[2][i] != layer_2_shape_ui[i].value for i in range(3)])) else '❌'}\n",
    "\n",
    "    After the second pooling layer $B \\times$ {layer_3_shape_ui[0].value} $\\times$ {layer_3_shape_ui[1].value} $\\times$ {layer_3_shape_ui[2].value}\n",
    "    {'✅' if (not any([layer_shape[3][i] != layer_3_shape_ui[i].value for i in range(3)])) else '❌'}   \n",
    "\n",
    "    After the first fully connected layer $B \\times$ {layer_4_shape_ui[0].value}\n",
    "    {'✅' if (not any([layer_shape[4][i] != layer_4_shape_ui[i].value for i in range(1)])) else '❌'}    \n",
    "\n",
    "    After the second fully connected layer $B \\times$ {layer_5_shape_ui[0].value}\n",
    "    {'✅' if (not any([layer_shape[5][i] != layer_5_shape_ui[i].value for i in range(1)])) else '❌'}     \n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We can improve the architecture by adding two more common operations: Batch Normalization and Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        # Convolutional Layer 1 with BatchNorm\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Convolutional Layer 2 with BatchNorm\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)  # Batch Normalization after Conv2\n",
    "\n",
    "        # Third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            32, 64, kernel_size=3, padding=1\n",
    "        )  # Output channels = 64\n",
    "        self.bn3 = nn.BatchNorm2d(64)  # Batch Normalization after Conv3\n",
    "\n",
    "        # Fourth convolutional layer\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            64, 128, kernel_size=3, padding=1\n",
    "        )  # Output channels = 128\n",
    "        self.bn4 = nn.BatchNorm2d(128)  # Batch Normalization after Conv4\n",
    "\n",
    "        # Fully connected layers with Dropout\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2, 128)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout with 50% probability\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conf and pooling layers\n",
    "        x = self.pool(\n",
    "            F.relu(self.bn1(self.conv1(x)))\n",
    "        )  # Conv -> BatchNorm -> ReLU -> Pool\n",
    "        x = self.pool(\n",
    "            F.relu(self.bn2(self.conv2(x)))\n",
    "        )  # Conv -> BatchNorm -> ReLU -> Pool\n",
    "        x = self.pool(\n",
    "            F.relu(self.bn3(self.conv3(x)))\n",
    "        )  # Conv -> BatchNorm -> ReLU -> Pool\n",
    "        x = self.pool(\n",
    "            F.relu(self.bn4(self.conv4(x)))\n",
    "        )  # Conv -> BatchNorm -> ReLU -> Pool\n",
    "\n",
    "        # Flatten the feature map\n",
    "        x = x.view(-1, 128 * 2 * 2)\n",
    "\n",
    "        # Fully connected layer 1 with Dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Fully connected layer 2 (output)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Training the model\n",
    "\n",
    "The training is exactly the same as for our Fully Connected Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from mofresh import refresh_matplotlib, ImageRefreshWidget\n",
    "import polars as pl\n",
    "\n",
    "widget = ImageRefreshWidget(src=\"\")\n",
    "\n",
    "@refresh_matplotlib\n",
    "def losschart(data):\n",
    "    df = pl.DataFrame(data)\n",
    "    plt.plot(df[\"epoch\"], df[\"train_loss\"])\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "datalogs = []\n",
    "\n",
    "# Train the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    running_correct, running_total = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    train_loader_with_progress = tqdm(\n",
    "        iterable=train_loader, ncols=120, desc=f\"Epoch {epoch+1}/{EPOCHS}\"\n",
    "    )\n",
    "    for batch_number, (inputs, labels) in enumerate(train_loader_with_progress):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # predicted = torch.argmax(outputs.data)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log data for tracking\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        running_total += labels.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_number % 100 == 99:\n",
    "            train_loader_with_progress.set_postfix(\n",
    "                {\n",
    "                    \"avg accuracy\": f\"{running_correct/running_total:.3f}\",\n",
    "                    \"avg loss\": f\"{running_loss/(batch_number+1):.4f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            datalogs.append(\n",
    "                {\n",
    "                    \"epoch\": epoch + batch_number / len(train_loader),\n",
    "                    \"train_loss\": running_loss / (batch_number + 1),\n",
    "                    \"train_accuracy\": running_correct / running_total,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    datalogs.append(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": running_loss / len(train_loader),\n",
    "            \"train_accuracy\": running_correct / running_total,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    widget.src = losschart(datalogs)\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for test_images, test_labels in test_loader:\n",
    "        test_images = test_images.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "        test_outputs = model(test_images)\n",
    "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "        test_total += test_labels.size(0)\n",
    "        test_correct += (test_predicted == test_labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Accuracy of the network on the 10000 test images: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "mo.md(rf\"\"\"The model has an **{test_accuracy:.2f}%** on the test set, much better than the accuracy we got from a fully connected network and due to parameter sharing of convolutional layers the CNN we constructed has less parameters!\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "_images, _labels = next(iter(test_loader))\n",
    "_images = _images.to(device)\n",
    "_labels = _labels.to(device)\n",
    "_outputs = model(_images).to(device)\n",
    "_, preds = torch.max(_outputs, 1)\n",
    "preds_single = CLASSES[preds.cpu().numpy()]\n",
    "actual_single = CLASSES[_labels.cpu().numpy()]\n",
    "n_to_show = 10\n",
    "indices = np.random.choice(range(len(_images)), n_to_show)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 3))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for _i, idx in enumerate(indices):\n",
    "    img = _images[idx].cpu().numpy().transpose((1, 2, 0))\n",
    "    ax = fig.add_subplot(1, n_to_show, _i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        -0.35,\n",
    "        \"pred = \" + str(preds_single[idx]),\n",
    "        fontsize=10,\n",
    "        ha=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        -0.7,\n",
    "        \"act = \" + str(actual_single[idx]),\n",
    "        fontsize=10,\n",
    "        ha=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
