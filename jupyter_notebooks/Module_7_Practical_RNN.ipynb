{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 7: Practical - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's improve the simple bigram text generation model we constructed in Module 1. First we construct the vocabulary from 10000 most frequently occuring words, assigning positions based on the frequency of occurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check which GPU is available\n",
    "device = (\n",
    "    torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess Count of Monte Cristo\n",
    "url = \"https://www.gutenberg.org/cache/epub/1184/pg1184.txt\"\n",
    "#url = \"https://www.fulltextarchive.com/book/the-count-of-monte-cristo/index.php\"\n",
    "\n",
    "import requests\n",
    "text = requests.get(url).text\n",
    "\n",
    "# Keep only the main body (remove header/footer)\n",
    "start_idx = text.find(\"Chapter 1\")\n",
    "end_idx = text.rfind(\"Chapter 5\") # text.rfind(\"End of the Project Gutenberg\")\n",
    "text = text[start_idx:end_idx]\n",
    "\n",
    "# Pre-processing\n",
    "text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenization\n",
    "tokens = text.split()\n",
    "\n",
    "# Vocabulary construction\n",
    "from collections import Counter\n",
    "counter = Counter(tokens)\n",
    "\n",
    "# We'll assign indices 0 and 1 to special tokens \"<PAD>\" and \"<UNK>\", the rest of the indeces\n",
    "# are based on the frequency of the words.\n",
    "vocab = {word: idx+2 for idx, (word, _) in enumerate(counter.most_common(9998))}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Encode tokens\n",
    "encoded = [vocab.get(word, vocab[\"<UNK>\"]) for word in tokens]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Since we are training the model to predict the next word in a sequence, we will construct our training set features based on 30 word sequences from the text. The corresponding labels are the sequences shifted by one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "SEQ_LEN = 30\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - SEQ_LEN\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.data[idx:idx+SEQ_LEN]),\n",
    "                torch.tensor(self.data[idx+1:idx+SEQ_LEN+1]))\n",
    "\n",
    "train_datasets = TextDataset(encoded)\n",
    "train_loader = DataLoader(train_datasets, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's see what the first pair of input/output sequences look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We now define the LSTM architecture. Since the LSTM layer is already implemented in PyTorch, we can use it directly. The LSTM layer takes the input sequence and returns the output sequence along with the hidden state. The output is then passed through a fully connected layer to get the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, embedding_dim=100, hidden_dim=128):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden\n",
    "\n",
    "model = LSTMModel(vocab_size=2400).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Finally, we train the model for 15 epochs. The loss should decrease over time, indicating that the model is learning to predict the next word in the sequence. After training, we can generate text by providing a seed phrase and letting the model predict the next words based on the learned patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    train_loader_with_progress = tqdm(\n",
    "        iterable=train_loader, ncols=120, desc=f\"Epoch {epoch+1}/{EPOCHS}\"\n",
    "    )\n",
    "    for batch_number, (inputs, targets) in enumerate(train_loader_with_progress):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs, _ = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_number % 100 == 0) or (batch_number == len(train_loader) - 1):\n",
    "            train_loader_with_progress.set_postfix(\n",
    "                {\n",
    "                    \"avg loss\": f\"{total_loss/(batch_number+1):.4f}\",\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, length=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = seed_text.lower().split()\n",
    "    input_ids = [vocab.get(w, vocab[\"<UNK>\"]) for w in words]\n",
    "    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            logits = output[0, -1] / temperature\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            words.append(inv_vocab.get(next_id, \"<UNK>\"))\n",
    "\n",
    "            # Extend input sequence with new token\n",
    "            input_ids.append(next_id)\n",
    "            input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"the sailboat\"\n",
    "vocab.get(seed)\n",
    "print(\"\\nGenerated Text:\\n\")\n",
    "print(generate_text(model, seed, length=150, temperature=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
