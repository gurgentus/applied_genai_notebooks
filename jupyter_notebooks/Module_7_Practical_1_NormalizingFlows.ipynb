{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 7: Practical - Normalizing Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Normalizing flows solve the problem of evaluating the likelyhood function by modeling the mapping from the latent distribution using a form, whose likelihood and parameters are tractable to calculate.\n",
    "\n",
    "$z = f(x)$\n",
    "\n",
    "$p_{data}(x) = p_{latent}(z)|J(x)|$\n",
    "\n",
    "$Loss$ =\n",
    "\n",
    "$-\\mathbb{E}_{x \\sim p_{data}} (\\log(p_{data}(x))) =$\n",
    "\n",
    "$-\\sum_{i=1}^N \\log( p_{model}(x_i | w ) = -\\sum_{i=1}^N \\log\\Big( p_{latent}(z_i | w ) |J(x_i)|\\Big) =$\n",
    "\n",
    "$-\\sum_{i=1}^N \\Big( \\log( p_{latent}(f(x_i | w )) + \\log(|J(x_i)|) \\Big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "# Load and preprocess Count of Monte Cristo\n",
    "url = \"https://www.gutenberg.org/cache/epub/1184/pg1184.txt\"\n",
    "\n",
    "import requests\n",
    "text = requests.get(url).text\n",
    "\n",
    "# Keep only the main body (remove header/footer)\n",
    "start_idx = text.find(\"Chapter 1.\")\n",
    "end_idx = text.rfind(\"Chapter 5.\") # text.rfind(\"End of the Project Gutenberg\")\n",
    "text = text[start_idx:end_idx]\n",
    "\n",
    "# Pre-processing\n",
    "text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenization\n",
    "tokens = text.split()\n",
    "\n",
    "# Vocabulary construction\n",
    "from collections import Counter\n",
    "counter = Counter(tokens)\n",
    "\n",
    "# We'll assign indices 0 and 1 to special tokens \"<PAD>\" and \"<UNK>\", the rest of the indeces\n",
    "# are based on the frequency of the words.\n",
    "vocab = {word: idx+2 for idx, (word, _) in enumerate(counter.most_common(9998))}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Encode tokens\n",
    "encoded = [vocab.get(word, vocab[\"<UNK>\"]) for word in tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Since we are training the model to predict the next word in a sequence, we will construct our training set features based on 30 word sequences from the text. The corresponding labels are the sequences shifted by one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "SEQ_LEN = 30\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - SEQ_LEN\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.data[idx:idx+SEQ_LEN]),\n",
    "                torch.tensor(self.data[idx+1:idx+SEQ_LEN+1]))\n",
    "\n",
    "train_datasets = TextDataset(encoded)\n",
    "train_loader = DataLoader(train_datasets, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's see what the first pair of input/output sequences look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We now define the LSTM architecture. Since the LSTM layer is already implemented in PyTorch, we can use it directly. The LSTM layer takes the input sequence and returns the output sequence along with the hidden state. The output is then passed through a fully connected layer to get the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, embedding_dim=100, hidden_dim=128):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden\n",
    "\n",
    "model = LSTMModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Finally, we train the model for 15 epochs. The loss should decrease over time, indicating that the model is learning to predict the next word in the sequence. After training, we can generate text by providing a seed phrase and letting the model predict the next words based on the learned patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {},
   "source": [
    "Finally we generate new text by feeding a seed sentence to the model. We then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, length=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = seed_text.lower().split()\n",
    "    input_ids = [vocab.get(w, vocab[\"<UNK>\"]) for w in words]\n",
    "    input_tensor = torch.tensor(input_ids).unsqueeze(0)\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            logits = output[0, -1] / temperature\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            words.append(inv_vocab.get(next_id, \"<UNK>\"))\n",
    "\n",
    "            # Extend input sequence with new token\n",
    "            input_ids.append(next_id)\n",
    "            input_tensor = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"the count of monte cristo\"\n",
    "print(\"\\nGenerated Text:\\n\")\n",
    "print(generate_text(model, seed, length=50))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}