{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 11 - Applying Reinforcement Learning to Language\n",
    "\n",
    "The goal of this activity is to build intuition on how Reinforcement Learning can modify the behavior of a language model. This is a very simplified toy example, but will help us understand how the RL concepts like **states** and **actions** translate to the world of text and what are some of the tradeoffs that need to be made when working with reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's first construct a simplified language model. Our model will have a set vocabulary size **vocab_size**, so for simplicity we'll use a subset of **vocab_size** words from the vocabulary of english words in the **nltk** library ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Download word list (only needs to be done once)\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "def build_a_vocab(vocab_size=200, min_len=3, max_len=8, seed=42):\n",
    "    all_words = words.words()\n",
    "    filtered = [\n",
    "        w.lower() for w in all_words\n",
    "        if w.isalpha() and min_len <= len(w) <= max_len\n",
    "    ]\n",
    "    unique = sorted(set(filtered))\n",
    "    random.seed(seed)\n",
    "    selected = random.sample(unique, vocab_size)\n",
    "    return selected\n",
    "\n",
    "vocab = build_a_vocab(vocab_size=1000)\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "\n",
    "print(\"Sample vocab:\", vocab[:10])\n",
    "print(\"Total words:\", len(vocab))\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "To simulate a language model generating text, we'll use a simple model that generates the next word starting with the last letter of the context. We will use a small one layer neural network that outputs probabilities for different words in the vocabulary and then choose the word with the highest probability that satisfies the word chain condition above.\n",
    "\n",
    "To obtain a numeric tensor, we will use one-hot encoding to represent the context, where each word in the vocabulary is represented by a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple one layer neural network that will act as our language model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.activation(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# our neural network takes a tensor as input, so we need to transform the\n",
    "# context into a tensor of one-hot encoded vectors.\n",
    "def encode_sequence_state(context, context_size=3):\n",
    "    # Encodes the context as a one-hot vector of size (context_size * vocab_size)\n",
    "    vec = torch.zeros(context_size * vocab_size, dtype=torch.float32)\n",
    "    # Ensure the context is at least as long as context_size\n",
    "    last_k = context[-context_size:] if len(context) >= context_size else context\n",
    "    # Offset determines which word in the context to start filling in\n",
    "    offset = context_size - len(last_k)\n",
    "\n",
    "    for i, idx in enumerate(last_k):\n",
    "        vec[(offset + i) * vocab_size + idx] = 1.0\n",
    "    return vec\n",
    "\n",
    "# checks if the word chain condition is satisfied, i.e. the first\n",
    "# letter of the current word matches the last letter of the previous word.\n",
    "def is_valid_transition(prev, curr):\n",
    "    return curr[0] == prev[-1]\n",
    "\n",
    "# to generate text from the base model, we need to encode the context,\n",
    "# get the probabilities for the next word logits, and sample from the distribution\n",
    "def generate_text_from_base_model(context, base_model):\n",
    "    encoded_context = encode_sequence_state(context).unsqueeze(0)\n",
    "    next_word_logits = base_model(encoded_context)\n",
    "\n",
    "    admissible_idxs = [i for i, w in enumerate(vocab) if is_valid_transition(idx_to_word[context[-1]], w)]\n",
    "    next_word_probabilities = F.softmax(next_word_logits[0][admissible_idxs], dim=-1)\n",
    "    next_word_dist = torch.distributions.Categorical(next_word_probabilities)\n",
    "    next_word_sample = next_word_dist.sample()\n",
    "    return admissible_idxs[next_word_sample.item()], next_word_dist.log_prob(next_word_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Feel free to expertiment with trying different context below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SimpleNet(input_dim=vocab_size * 3, hidden_dim=64, action_dim=vocab_size)\n",
    "\n",
    "context = [10, 23, 40]\n",
    "print(\"Context: \", [idx_to_word[ind] for ind in context])\n",
    "completion = []\n",
    "for _i in range(5):\n",
    "    next_word_index, _  = generate_text_from_base_model(context, base_model)\n",
    "    context.append(next_word_index)\n",
    "    completion.append(idx_to_word[next_word_index])\n",
    "print(\"Completion: \", ', '.join(completion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "This is the base model behavior. What happens if we want to modify it? Say we want to generate new text in such a way that the last word in the chain ends in the letter **m**.\n",
    "\n",
    "We could train a new model or fine-tune the existing model, but that would require many examples of this behavior. Can we make our text generation behave in the way we want by simply having it experiment on its own? All we would need would be some rules to tell it if it is doing a good job. This is a perfect setup for using Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's put this problem in an RL setting of states, actions, and rewards. The **state** will be the current context of length **context_size**.\n",
    "\n",
    "The **action** is simply choosing one of the words from the vocabulary.\n",
    "\n",
    "The **reward** is where things get interesting. The idea is to shape it in a way to get the behavior we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaped_reward(sequence):\n",
    "    reward = 0.0\n",
    "    for i in range(1, len(sequence)):\n",
    "        if is_valid_transition(sequence[i - 1], sequence[i]):\n",
    "            reward += 1.0\n",
    "        else:\n",
    "            reward -= 3.0\n",
    "\n",
    "    if sequence[-1][-1] == 'm':\n",
    "        reward += 50\n",
    "    else:\n",
    "        reward -= 10\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Finally, the **environment** will be represented by a class that holds the current state of the text generation process and provides methods to reset the state and step through the environment by taking an action (choosing a word). The environment also computes the reward based on the generated sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, max_seq_length):   \n",
    "        self.state = []\n",
    "        self.context_size = 3\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def reset(self, start_word=None):\n",
    "        start_idx = word_to_idx[start_word] if start_word else word_to_idx[random.choice(vocab)]\n",
    "        self.state = [start_idx]\n",
    "        context = self.state[-3:]\n",
    "        return context\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state = list(self.state) + [action]\n",
    "        new_context = self.state[-3:]\n",
    "        reward = 0;\n",
    "        done = False\n",
    "        if ((idx_to_word[self.state[-1]][-1] == 'm') or len(self.state) == self.max_seq_length):\n",
    "            done = True\n",
    "            sequence = [idx_to_word[i] for i in self.state]\n",
    "            reward = shaped_reward(sequence)\n",
    "\n",
    "        return new_context, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(max_seq_length=5)\n",
    "_context = env.reset(\"strait\")\n",
    "print(\"Current context: \", [idx_to_word[word] for word in _context])\n",
    "\n",
    "# one step in the environment\n",
    "_new_context, _reward, _done = env.step(word_to_idx[\"group\"])\n",
    "print(\"New context: \", [idx_to_word[word] for word in _new_context])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "How should the agent choose the next action (next word)? Here is an example of a policy implementation based on the algorithm described and implemented in https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html.\n",
    "\n",
    "The `get_action` function simply uses the neural network model to output the next action, as we did before. The details of the training algorithm below and the specific loss function used for RL is beyond the scope of the course, but note that it is a generic algorithm for RL, not specific for language modeling.\n",
    "\n",
    "Pay particular attention to the last several lines where the usual backpropogation to perform gradient descent with respect to this loss function is used to update the weights of the base neural network. In case you are interested in diving more into the theory, the code in this cell is copied with minimal modifications from above to make the comparison with the theory easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, model, optimizer, batch_size):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(self, obs):  \n",
    "        return generate_text_from_base_model(obs, self.model)\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(self, logp, weights):\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch(self):\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "        batch_logp = []\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "            # save obs\n",
    "            batch_obs.append(obs)\n",
    "\n",
    "            # act in the environment\n",
    "            act, logp = self.get_action(obs)\n",
    "            obs, rew, done = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "            batch_logp.append(logp)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > self.batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_loss = self.compute_loss(logp,\n",
    "                            weights=torch.as_tensor(batch_weights, dtype=torch.float32))\n",
    "        batch_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Next, let's train the model. Each epoch consists of `batch_size` of observations, where each episode is a sequence of actions taken by the agent in the environment until the end condition. The agent interacts with the environment, collects rewards, and updates its policy based on the rewards received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 30\n",
    "\n",
    "lr=1e-2\n",
    "input_dim = 3 * vocab_size\n",
    "policy_nn = SimpleNet(input_dim, hidden_dim=256, action_dim=vocab_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_nn.parameters(), lr=lr)\n",
    "policy = Policy(policy_nn, optimizer, batch_size)\n",
    "\n",
    "# training loop\n",
    "reward_history = []\n",
    "for epoch in range(epochs):\n",
    "    batch_loss, batch_rets, batch_lens = policy.train_one_epoch()\n",
    "    reward_history.append(np.mean(batch_rets))\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "            (epoch, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(rewards, alpha=0.4, label=\"Raw reward\")\n",
    "    if len(rewards) >= 100:\n",
    "        smoothed = torch.tensor(rewards).unfold(0, 100, 1).mean(1)\n",
    "        plt.plot(range(99, len(rewards)), smoothed, label=\"Smoothed (100 ep)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(reward_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "The model is now trained and we can generate new text! Note, how our reward shape affected the generation. Feel free to experiment with increasing vocabulary size, maximum sequence length and the reward shape (e.g. can we reward the model for making longer or shorter chains or having some other interesting structure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chain(start_word, policy):\n",
    "    # make some empty lists for logging.\n",
    "        batch_observations = []          # for observations\n",
    "        # reset episode-specific variables\n",
    "        context = env.reset(start_word)\n",
    "        done = False            # signal from environment that episode is over\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "            # act in the environment\n",
    "            action, _ = policy.get_action(context)\n",
    "            context, _, done = env.step(action)\n",
    "            batch_observations.append(idx_to_word[context[-1]])\n",
    "            if done:\n",
    "                return batch_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_word = random.choice(vocab)\n",
    "print(\"Start word: \", start_word)\n",
    "\n",
    "chain = generate_chain(start_word, policy)\n",
    "print(\" → \".join(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
