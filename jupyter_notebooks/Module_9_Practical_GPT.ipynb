{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\u26a0\ufe0f **Static Version Notice**\n",
    "\n",
    "This is a static export of an interactive marimo notebook. Some features have been modified for compatibility:\n",
    "\n",
    "- Interactive UI elements (sliders, dropdowns, text inputs) have been removed\n",
    "- UI variable references have been replaced with default values\n",
    "- Some cells may have been simplified or removed entirely\n",
    "\n",
    "For the full interactive experience, please run the original marimo notebook (.py file) using:\n",
    "```bash\n",
    "uv run marimo edit notebook_name.py\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 9: Practical - Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We start with the same data preparation steps as in Module 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "# Load and preprocess Count of Monte Cristo\n",
    "url = \"https://www.gutenberg.org/cache/epub/1184/pg1184.txt\"\n",
    "\n",
    "import requests\n",
    "text = requests.get(url).text\n",
    "\n",
    "# Keep only the main body (remove header/footer)\n",
    "start_idx = text.find(\"Chapter 1.\")\n",
    "end_idx = text.rfind(\"Chapter 5.\") # text.rfind(\"End of the Project Gutenberg\")\n",
    "text = text[start_idx:end_idx]\n",
    "\n",
    "# Pre-processing\n",
    "text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenization\n",
    "tokens = text.split()\n",
    "\n",
    "# Vocabulary construction\n",
    "from collections import Counter\n",
    "counter = Counter(tokens)\n",
    "\n",
    "# We'll assign indices 0 and 1 to special tokens \"<PAD>\" and \"<UNK>\", the rest of the indeces\n",
    "# are based on the frequency of the words.\n",
    "vocab = {word: idx+2 for idx, (word, _) in enumerate(counter.most_common(9998))}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Encode tokens\n",
    "encoded = [vocab.get(word, vocab[\"<UNK>\"]) for word in tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Since we are training the model to predict the next word in a sequence, we will construct our training set features based on 30 word sequences from the text. The corresponding labels are the sequences shifted by one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "SEQ_LEN = 30\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - SEQ_LEN\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "                torch.tensor(self.data[idx+1:idx+SEQ_LEN+1]))\n",
    "\n",
    "train_datasets = TextDataset(encoded)\n",
    "train_loader = DataLoader(train_datasets, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's see what the first pair of input/output sequences look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We now define the causal attention mask.  Recall that this mask simply zeroes out the attention weights for future tokens in the sequence. This is done to ensure that the model does not have access to future tokens when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(n_dest, n_src, device):\n",
    "    i = torch.arange(n_dest, device=device).unsqueeze(1)\n",
    "    j = torch.arange(n_src, device=device).unsqueeze(0)\n",
    "    return i >= j\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "mask = causal_attention_mask(10, 10, device)\n",
    "print(mask[0].T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Recall that we also need to define a position embedding.  Here we will use a simple positional encoding corresponding to the embedding of the index of the token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        pos_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(x)\n",
    "        return token_embeddings + pos_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Next we define the Transformer block, consisting of, in addition to the usual fully connected layers, also multi-head attention and layer normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ln_1 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout_1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.ln_2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        causal_mask = causal_attention_mask(seq_len, seq_len, x.device)\n",
    "        # causal_mask = causal_mask.unsqueeze(1)  # for broadcasting\n",
    "        attn_output, attn_weights = self.attn(x, x, x, attn_mask=~causal_mask.bool())\n",
    "        x = self.ln_1(x + self.dropout_1(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.ln_2(x + ffn_output)\n",
    "        return x, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Finally, let's put it all together into a GPT (Generative Pre-trained Transformer) architecture and train the model using the dataloader defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-style transformer wrapper\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, max_len, vocab_size, embed_dim, num_heads, key_dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.embed = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim)\n",
    "        self.transformer = TransformerBlock(num_heads, key_dim, embed_dim, ff_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x, attn_weights = self.transformer(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_gpt(model, dataloader, optimizer, criterion, epochs, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        data_loader_with_progress = tqdm(\n",
    "            iterable=dataloader, ncols=120, desc=f\"Epoch {epoch+1}/{epochs}\"\n",
    "        )\n",
    "        for batch_number, (inputs, targets) in enumerate(data_loader_with_progress):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(inputs)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if (batch_number % 100 == 0) or (batch_number == len(dataloader) - 1):\n",
    "                data_loader_with_progress.set_postfix(\n",
    "                    {\n",
    "                        \"avg loss\": f\"{total_loss/(batch_number+1):.4f}\",\n",
    "                    }\n",
    "                )            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We can now use the trained GPT to generate text.  The model will generate a sequence of tokens based on the input prompt. We can use the inverse mapping from our vocabulary to \"translate\" the tokens to natural text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, model, index_to_word, top_k=10):\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {word: idx for idx, word in enumerate(index_to_word)}\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs[1] = 0  # Mask out UNK token (index 1) to prevent generating <UNK>\n",
    "        probs = torch.nn.functional.softmax(probs/temperature, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        return next_id, probs\n",
    "\n",
    "    def generate(self, start_prompt, max_tokens, temperature):\n",
    "        self.model.eval()\n",
    "        start_tokens = [self.word_to_index.get(w, 1) for w in start_prompt.split()]\n",
    "        generated_tokens = start_tokens[:]\n",
    "        info = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while len(generated_tokens) < max_tokens:\n",
    "                x = torch.tensor([generated_tokens], dtype=torch.long)\n",
    "                x = x.to(device)\n",
    "                logits, attn_weights = self.model(x)\n",
    "                last_logits = logits[0, -1] # .cpu().numpy()\n",
    "                sample_token, probs = self.sample_from(last_logits, temperature)\n",
    "                generated_tokens.append(sample_token)\n",
    "                info.append({\n",
    "                    \"prompt\": start_prompt,\n",
    "                    \"word_probs\": probs,\n",
    "                    \"atts\": attn_weights[0].cpu().numpy()\n",
    "                })\n",
    "                if sample_token == 0:\n",
    "                    break\n",
    "        print(\"GEN\", generated_tokens)\n",
    "        generated_words = [self.index_to_word.get(idx, \"<UNK>\") for idx in generated_tokens]\n",
    "        print(\"generated text:\" + \" \".join(generated_words))\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = TextGenerator(model, inv_vocab)\n",
    "info = text_generator.generate(\"captain \", max_tokens=180, temperature=3.0)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}