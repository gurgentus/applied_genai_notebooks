{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 5: Practical - Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which GPU is available\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "The images in the Fashion-MNIST dataset that we will use are 28 by 28 pixel images with pixel values normalized to be between 0 and 1. While we can design our neural network to take in 28 by 28 image, it is easier to first pad the images with zeros to obtain a 32 by 32 pixel image. Since the standard convolution layers half the size of the image, having width and height equal a power of 2 allows us to process the input through three convolution layers without any extra code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Data\n",
    "\n",
    "# Define custom preprocessing function\n",
    "def preprocess(img):\n",
    "    img = np.pad(img, ((2, 2), (2, 2)), constant_values=0.0)\n",
    "    return img;\n",
    "\n",
    "transform = transforms.Compose([transforms.Lambda(preprocess), transforms.ToTensor()])\n",
    "# Alternatively\n",
    "# transform = transforms.Compose([transforms.Pad(2), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's look at the shape of the first image in the train dataset. As we can see the image is 32 by 32 pixels with one channel (grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image, first_label = train_dataset[0]\n",
    "print(first_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's visualize it using `plt.imshow`. Recall that it expects the channel to be the last dimension, so we permute the image first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label: \", first_label)\n",
    "plt.imshow(first_image.permute(1,2,0).squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of the train dataset: \", len(train_dataset))\n",
    "print(\"Length of list(train_loader): \", len(list(train_loader)))\n",
    "print(\"Type of the first element of list(train_loader): \", type(list(train_loader)[0]))\n",
    "print(\"Type of the first element of list(train_loader)[0]: \", type(list(train_loader)[0][0]))\n",
    "print(\"Shape of the first element of list(train_loader)[0]: \", list(train_loader)[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's look at the first image in the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_batch_images, next_batch_labels = next(iter(train_loader))\n",
    "_first_img = next_batch_images[0] # retrieve the first image from the batch of 32\n",
    "_first_label = next_batch_labels[0] # retrieve the first label from the batch of 32\n",
    "plt.imshow(_first_img.permute(1, 2, 0), cmap='gray') # imshow requires the image to be in height x width x channels format\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Autoencoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We can now define our Encoder architecture, which consists of three convolutional layers followed by a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(128 * 4 * 4, 2)  # Adjusted for input size (32,32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Similarly, the decoder architecture replaces the convolutional layers with convolutional transpose layers, which are applied after a fully connected layer, 'reversing' the operations in the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(2, 128 * 4 * 4)\n",
    "        self.convtrans1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convtrans2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convtrans3 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 128, 4, 4)\n",
    "        x = F.relu(self.convtrans1(x))\n",
    "        x = F.relu(self.convtrans2(x))\n",
    "        x = torch.sigmoid(self.convtrans3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Finally, the autoencoder forward pass simply applies the encoder followed by decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "As usual, we define the model, the optimizer, and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters\n",
    "EPOCHS = 10\n",
    "\n",
    "def train_model(model, optimizer, loss_function, train_loader, device, epochs=10):\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        # tqdm is a nice library to use to show a progress bar for our training\n",
    "        # To use it we simply replace:\n",
    "        #   for data in train loader\n",
    "        # with:\n",
    "        train_loader_with_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for data in train_loader_with_progress:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            train_loader_with_progress.set_postfix(loss=f'{loss.item():.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "train_model(autoencoder, optimizer, loss_function, train_loader, device, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's now load the first batch from the test data loader and run the autoencoder model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first batch of images from the test dataset\n",
    "example_images, _ = next(iter(test_loader))\n",
    "example_images = example_images.to(device)\n",
    "\n",
    "# Perform predictions\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = autoencoder(example_images)\n",
    "\n",
    "print(\"Predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Recall that the prediction is the model's reconstruction of the input image from the latent representation generated by the encoder. Let's visualize the first reconstruction. As always, we need to move the pytorch image tensors to the cpu and convert them to numpy before we can visualize them.  Since the images are grayscale we also remove the single channel dimension using `squeeze()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = example_images[1].cpu().squeeze().numpy()\n",
    "reconstructed = predictions[1].cpu().squeeze().numpy()\n",
    "\n",
    "# Plotting the images side by side\n",
    "_fig, _axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "_axes[0].imshow(original, cmap='gray')\n",
    "_axes[0].set_title(\"Original\")\n",
    "_axes[0].axis('off')\n",
    "\n",
    "_axes[1].imshow(reconstructed, cmap='gray')\n",
    "_axes[1].set_title(\"Reconstructed\")\n",
    "_axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AjVT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We will now generate new images by first sampling in the latent space and then running the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate embeddings from your test data\n",
    "autoencoder.eval()\n",
    "embeddings = []\n",
    "\n",
    "# Perform predictions\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    embedding = autoencoder.encoder(example_images).cpu().numpy()\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# Generate random points in embedding space within the bounds of the encoded first batch of images.\n",
    "mins, maxs = np.min(embeddings, axis=0), np.max(embeddings, axis=0)\n",
    "random_samples = np.random.uniform(mins, maxs, size=(18, 2))\n",
    "random_samples = torch.tensor(random_samples, dtype=torch.float32).to(device)\n",
    "\n",
    "# Decode random samples\n",
    "with torch.no_grad():\n",
    "    reconstructions = autoencoder.decoder(random_samples).cpu().numpy()\n",
    "\n",
    "# Plot reconstructed images\n",
    "_fig, _axes = plt.subplots(3, 6, figsize=(12, 6))\n",
    "for _i, _ax in enumerate(_axes.flat):\n",
    "    _ax.imshow(reconstructions[_i].squeeze(), cmap='gray')\n",
    "    _ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NCOB",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "The results are not bad, but some of the images are not very realistic. One of the reasons is that the encoder does not enforce any continuity conditions. Two points, which are very close in the latent space might not actually correspond to similar images. This issue is not too bad with a smaller dimensional embedding space, but as we move to more complicated images and higher dimensional latent representations, this will become worse - this is the 'curse of dimensionality'.  To address this, we will now implement a Variational Autoencoder (VAE) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Variational Autoencoder (VAE) Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TRpd",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "The idea behind VAE is for the encoder to map to a distribution, rather than an exact latent variable. So the encoder network now outputs two vectors, the mean and the log variance of a normal distribution.\n",
    "In turn, the decoder, first samples from this distribution and then uses the same decoder as before to map the sampled latent variable back to the original image space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar   \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, beta=1.0):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction, mu, logvar\n",
    "\n",
    "# Example usage\n",
    "encoder = VariationalEncoder()\n",
    "decoder = Decoder()\n",
    "vae = VAE(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dNNg",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We also need to slightly change the loss function.  In addition to penalizing the deviation of the reconstructed image from the original input, we penalize deviation of the normal distribution in the latent space from a standard normal distribution (mean: 0, variance 1) through the Kullback-Leibler (KL) divergence term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae_model(model, optimizer, loss_function, train_loader, device, epochs=10):\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        # tqdm is a nice library to use to show a progress bar for our training\n",
    "        # To use it we simply replace:\n",
    "        #   for data in train loader\n",
    "        # with:\n",
    "        train_loader_with_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for data in train_loader_with_progress:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            loss = loss_function(recon, inputs, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            train_loader_with_progress.set_postfix(loss=f'{loss.item():.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    beta = 500\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return beta * BCE + KLD\n",
    "\n",
    "_optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "train_vae_model(vae, _optimizer, vae_loss_function, train_loader, device, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlCL",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "num_samples = 20\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(num_samples, 2).to(device)\n",
    "    samples = vae.decoder(z)\n",
    "    samples = samples.cpu().numpy()\n",
    "\n",
    "_fig, _axes = plt.subplots(3, 6, figsize=(12, 6))\n",
    "for _i, _ax in enumerate(_axes.flat):\n",
    "    _ax.imshow(samples[_i].squeeze(), cmap='gray')\n",
    "    _ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wAgl",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
