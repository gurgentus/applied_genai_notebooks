{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68191779",
   "metadata": {},
   "source": [
    "⚠️ **Static Version Notice**\n",
    "\n",
    "This is a static export of an interactive marimo notebook. Some features have been modified for compatibility:\n",
    "\n",
    "- Interactive UI elements (sliders, dropdowns, text inputs) have been removed\n",
    "- UI variable references have been replaced with default values\n",
    "- Some cells may have been simplified or removed entirely\n",
    "\n",
    "For the full interactive experience, please run the original marimo notebook (.py file) using:\n",
    "```bash\n",
    "uv run marimo edit notebook_name.py\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df206f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "NUM_CLASSES = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42) \n",
    "np.random.seed(42)\n",
    "\n",
    "# Check which GPU is available\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 4: Practical 1 - Fully Connected Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Loss Functions\n",
    "\n",
    "We start with a brief review of Fully Connected Neural Networks. Recall that in Supervised Learning we tune the parameters of the neural network to minimize the \"Loss Function\" that represents how close the neural network outputs are to the labels defined in the dataset. There are many choices for loss functions, but the most common are Mean Square Loss for regression problems and Categorial Cross Entropy Loss for classification problems.\n",
    "\n",
    "#### **Mean Square Loss**\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "\n",
    "#### **Categorical Cross Entropy Loss**\n",
    "$-\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})$\n",
    "\n",
    "For the classification problems the network typically outputs $\\hat{y}_{i, c}$ the probability of the example $i$ belonging to the class $c$. In the formula above, $y_{i,c}$ is equal to $1$ for the correct class, and $0$ otherwise, so we are really just averaging the logarithms of the probability the network outputs for the correct label in the training data.\n",
    "\n",
    "  - Loss: $-\\log(p_{\\text{correct class}})$\n",
    "  - Directly uses the index of the correct class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Recall that each artificial neuron in a neural network takes a linear combination of its inputs followed by the application of an activation function. For classification problems, the `softmax` activation function turns the outputs of the final layer into probabilities summing up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Here is an example of what a simple Fully Connected Neural Network might looks like. Feel free to experiment with the inputs and the weights to see how they influence the output probabilities. Of course, the goal of training a neural network is to find the weights (coefficients) in each artificial neuron to minimize the loss function on the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "weather_categories = ['sunny', 'rainy', 'cloudy', 'snowy']\n",
    "# Create sample data for the network\n",
    "\n",
    "# Define input variables\n",
    "weather_examples = [\n",
    "    [28.5, 45.0, 1015.2],\n",
    "    [18.7, 78.3, 1008.5],\n",
    "    [22.1, 85.6, 998.7]\n",
    "]\n",
    "weather_categories = [\"Sunny\", \"Rainy\", \"Stormy\"]\n",
    "\n",
    "# Define input variables\n",
    "var_names = [\"bias\", \"temperature (°C)\", \"humidity (%)\", \"pressure (hPa)\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#[1, example[\"temp\"], example[\"humidity\"], example[\"pressure\"]]\n",
    "\n",
    "# Define weights (randomly initialized)\n",
    "w = np.round(np.random.uniform(-1, 1, size=(2, 4)), 2)  # 3 inputs × 2 hidden neurons\n",
    "# w = np.array([\n",
    "#     [ 0.3, -0.2,  0.1,  0.4],  # temperature weights\n",
    "#     [-0.1,  0.5,  0.2, -0.3],  # humidity weights\n",
    "#     [ 0.2,  0.1, -0.4,  0.1]   # pressure weights\n",
    "# ])\n",
    "\n",
    "v = np.round(np.random.uniform(-1, 1, size=(3, 4)), 2)       # 2 hidden neurons × 1 output\n",
    "# Weights for output layer (4 hidden neurons × 3 output classes)\n",
    "# v = np.array([\n",
    "#     [ 0.5,  0.1, -0.3],  # h1 to outputs\n",
    "#     [-0.2,  0.4,  0.1],  # h2 to outputs\n",
    "#     [ 0.1, -0.1,  0.5],  # h3 to outputs\n",
    "#     [-0.1,  0.3,  0.2]   # h4 to outputs\n",
    "# ])\n",
    "\n",
    "\n",
    "w[0,0] = 0.0\n",
    "w[0,1] = 0.0\n",
    "w[0,2] = 0.0\n",
    "w[0,3] = 0.0\n",
    "\n",
    "w[1,0] = 0.0\n",
    "w[1,1] = 0.0\n",
    "w[1,2] = 0.0\n",
    "w[1,3] = 0.0\n",
    "\n",
    "v[0,0] = 0.0\n",
    "v[0,1] = 0.0\n",
    "v[1,0] = 0.0\n",
    "v[1,1] = 0.0\n",
    "v[2,0] = 0.0\n",
    "v[2,1] = 0.0\n",
    "\n",
    "\n",
    "# Calculate hidden layer values\n",
    "h_inputs = np.zeros(2)\n",
    "for i in range(2):\n",
    "    h_inputs[i] = np.round(np.sum([w[i, j] * [1, 0.72, 0.34, 0.89][j] for j in range(4)]), 2)\n",
    "h_values = np.round(sigmoid(h_inputs), 2)\n",
    "\n",
    "# Calculate output values\n",
    "y_inputs = np.zeros(3)\n",
    "for i in range(3):\n",
    "    y_inputs[i] = np.round(np.sum([v[i, j] * [0.85, 0.23][j] for j in range(2)]), 2)\n",
    "y_values = np.round(softmax(y_inputs), 2) # Using softmax for multi-class classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "h_inputs = [1.73, -1.19]\n",
    "y_inputs = [-1.6, 0.8, -2.3]\n",
    "weather_categories = ['sunny', 'rainy', 'cloudy', 'snowy']\n",
    "import networkx as nx\n",
    "import math\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "gs = GridSpec(2, 3, height_ratios=[1, 3])\n",
    "\n",
    "# Main network diagram in top area (spanning 3 columns)\n",
    "ax_main = fig.add_subplot(gs[1, :])\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes with positions\n",
    "pos = {\n",
    "    \"x1\": (0, 0), \"x2\": (0, 0.5), \"x3\": (0, 1), \"bias\": (0, -0.5), # Input layer\n",
    "    \"h1\": (1.5, 0.25), \"h2\": (1.5, 0.75),            # Hidden layer\n",
    "    \"y1\": (3, 0), \"y2\": (3, 0.5), \"y3\": (3, 1)               # Output layer\n",
    "}\n",
    "\n",
    "# Add nodes\n",
    "G.add_nodes_from([\"x1\", \"x2\", \"x3\", \"bias\", \"h1\", \"h2\", \"y1\", \"y2\", \"y3\"])\n",
    "\n",
    "# Add edges\n",
    "edges = [\n",
    "    (\"x1\", \"h1\"), (\"x1\", \"h2\"), \n",
    "    (\"x2\", \"h1\"), (\"bias\", \"h1\"), (\"x2\", \"h2\"),\n",
    "    (\"x3\", \"h1\"), (\"x3\", \"h2\"),\n",
    "    (\"h1\", \"y1\"), (\"h2\", \"y1\"),\n",
    "    (\"h1\", \"y2\"), (\"h2\", \"y2\"),\n",
    "    (\"h1\", \"y3\"), (\"h2\", \"y3\")\n",
    "]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(G, pos, with_labels=False, node_size=3000, \n",
    "        node_color=[\"lightblue\", \"lightblue\", \"lightblue\", \"lightblue\", \"lightgreen\", \"lightgreen\", \"salmon\", \"salmon\", \"salmon\"],\n",
    "        arrowsize=20, arrowstyle='->', width=1.5, ax=ax_main)\n",
    "\n",
    "# Add custom node labels with interpolated values\n",
    "node_labels = {\n",
    "    \"bias\": \"1\",\n",
    "    \"x1\": f\"x₁\\n{[1, 0.72, 0.34, 0.89][1]}\", \n",
    "    \"x2\": f\"x₂\\n{[1, 0.72, 0.34, 0.89][2]}\", \n",
    "    \"x3\": f\"x₃\\n{[1, 0.72, 0.34, 0.89][3]}\", \n",
    "    \"h1\": f\"\\n\\nh₁\\n\\n\" + fr\"$\\sigma(input) = {[0.85, 0.23][0]}$\", \n",
    "    \"h2\": f\"\\n\\nh₂\\n\\n\" + fr\"$\\sigma(input) = {[0.85, 0.23][1]}$\", \n",
    "    \"y1\": f\"\\n\\ny₁\\n\\n ${[0.2, 0.7, 0.1][0]}$\",\n",
    "    \"y2\": f\"\\n\\ny₂\\n\\n ${[0.2, 0.7, 0.1][1]}$\",\n",
    "    \"y3\": f\"\\n\\ny₃\\n\\n ${[0.2, 0.7, 0.1][2]}$\"\n",
    "}\n",
    "nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=18, font_weight=\"bold\", ax=ax_main)\n",
    "\n",
    "# Add edge labels with actual weight values\n",
    "edge_labels = {\n",
    "    (\"bias\", \"h1\"): fr\"$\\times(w)$\", \n",
    "    (\"x1\", \"h1\"): fr\"$\\times(w)$\", \n",
    "    # (\"x1\", \"h2\"): fr\"$\\times(w)$\", \n",
    "    (\"x2\", \"h1\"): fr\"$\\times(w)$\", \n",
    "    # (\"x2\", \"h2\"): f\"w3=w\",\n",
    "    (\"x3\", \"h1\"): fr\"$\\times(w)$\", \n",
    "    # (\"x3\", \"h2\"): f\"w5=w\",\n",
    "    (\"h1\", \"y1\"): fr\"$\\times(w)$\", \n",
    "    (\"h2\", \"y1\"): fr\"$\\times(w)$\"\n",
    "}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=20, ax=ax_main)\n",
    "\n",
    "# Add layer titles\n",
    "ax_main.text(0, 1.5, \"INPUT LAYER\", fontsize=16, ha=\"center\", fontweight=\"bold\")\n",
    "ax_main.text(1.5, 1.5, \"HIDDEN LAYER\", fontsize=16, ha=\"center\", fontweight=\"bold\")\n",
    "ax_main.text(3, 1.5, \"OUTPUT LAYER\", fontsize=16, ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "ax_main.text(-0.5, 1, \"Pressure\", fontsize=16, ha=\"center\", style='italic')\n",
    "ax_main.text(-0.5, 0.5, \"Humidity\", fontsize=16, ha=\"center\", style='italic')\n",
    "ax_main.text(-0.5, 0, \"Temperature\", fontsize=16, ha=\"center\", style='italic')\n",
    "\n",
    "\n",
    "# Add activation function labels\n",
    "ax_main.text(1.5, -0.3, \"Activation: Sigmoid\", fontsize=12, ha=\"center\", style='italic')\n",
    "ax_main.text(3, -0.5, \"Activation: Softmax\", fontsize=12, ha=\"center\", style='italic')\n",
    "\n",
    "example = 0\n",
    "ax_main.text(3.5, 1, f\"{weather_categories[2]}\", fontsize=16, ha=\"center\", color = 'blue' if example == 2 else 'black')\n",
    "ax_main.text(3.5, 0.5, f\"{weather_categories[1]}\", fontsize=16, ha=\"center\", color = 'blue' if example == 1 else 'black')\n",
    "ax_main.text(3.5, 0, f\"{weather_categories[0]}\", fontsize=16, ha=\"center\", color = 'blue' if example == 0 else 'black')\n",
    "\n",
    "ax_main.text(2, -0.7, \"Loss: -log(0.7) = 0.357\", fontsize=20, ha=\"center\", style='italic')\n",
    "\n",
    "\n",
    "# Color-code the layers with background shapes\n",
    "input_layer = plt.Rectangle((-0.5, -0.3), 0.8, 1.6, fill=True, alpha=0.1, color='blue')\n",
    "hidden_layer = plt.Rectangle((1, -0.05), 1, 1.1, fill=True, alpha=0.1, color='green')\n",
    "output_layer = plt.Rectangle((2.6, -0.3), 2, 1.5, fill=True, alpha=0.1, color='red')\n",
    "\n",
    "ax_main.add_patch(input_layer)\n",
    "ax_main.add_patch(hidden_layer)\n",
    "ax_main.add_patch(output_layer)\n",
    "\n",
    "# Add a sigmoid activation function plot in the bottom area\n",
    "ax_sigmoid = fig.add_subplot(gs[0, 1:2])\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "# Plot the sigmoid function\n",
    "ax_sigmoid.plot(x, y, 'b-', linewidth=2)\n",
    "ax_sigmoid.set_title('Sigmoid Activation Function: σ(z) = 1/(1+e^(-z))', fontsize=14)\n",
    "ax_sigmoid.set_xlabel('Input (z)', fontsize=12)\n",
    "ax_sigmoid.set_ylabel('Output: σ(z)', fontsize=12)\n",
    "ax_sigmoid.grid(True, alpha=0.3)\n",
    "ax_sigmoid.set_xlim(-6, 6)\n",
    "ax_sigmoid.set_ylim(-0.1, 1.1)\n",
    "\n",
    "\n",
    "ax_main.text(3, 1.4, \"softmax(logits)\", fontsize=18, ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Clearly, we need a systematic way to calculate these weights. Below we go over a typical neural network setup and training in PyTorch for the problem of classification of images from the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "CLASSES = np.array(\n",
    "    [\n",
    "        \"airplane\",\n",
    "        \"automobile\",\n",
    "        \"bird\",\n",
    "        \"cat\",\n",
    "        \"deer\",\n",
    "        \"dog\",\n",
    "        \"frog\",\n",
    "        \"horse\",\n",
    "        \"ship\",\n",
    "        \"truck\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prepare the Data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_img, _label = train_dataset[0]\n",
    "print(\"Label: \", _label)\n",
    "print(\"Class: \", CLASSES[_label])\n",
    "plt.imshow(_img.permute(1,2,0).squeeze())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Note that each training example is a tuple containing the three dimensional image tensor (C by H by W) and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some individual pixel: \", train_dataset[54][0][1, 12, 13])\n",
    "print(\"Corresponding Label: \", train_dataset[54][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "_random_index = np.random.randint(len(train_dataset))\n",
    "_img, _label = train_dataset[_random_index]\n",
    "print(\"Label: \", _label)\n",
    "print(\"Class: \", CLASSES[_label])\n",
    "plt.imshow(_img.permute(1,2,0).squeeze())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "PyTorch has a special *DataLoader* class that takes care of some of the tedious details of constructing batches from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of the train dataset: \", len(train_dataset))\n",
    "print(\"Length of list(train_loader): \", len(list(train_loader)))\n",
    "print(\"Type of the first element of list(train_loader): \", type(list(train_loader)[0]))\n",
    "print(\"Type of the first element of list(train_loader)[0]: \", type(list(train_loader)[0][0]))\n",
    "print(\"Shape of the first element of list(train_loader)[0]: \", list(train_loader)[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pvdt",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Dataloaders** provide multiple ways to access the data, either by converting it into a **Python list** or by using an **iterable**.\n",
    "\n",
    "Using `list(train_loader)`, as we have, loads the **entire dataset into memory**, which can be **slow** and even **fail** when dealing with large datasets.\n",
    "\n",
    "Since **neural network training algorithms process data in batches**, it is more efficient to use an **iterator**. Instead of retrieving the first batch like this:\n",
    "```python\n",
    "list(train_loader)[0]\n",
    "```\n",
    "which loads everything into memory, we use:\n",
    "```python\n",
    "next(iter(train_loader))\n",
    "```\n",
    "This approach retrieves only the first batch without loading the entire dataset, making it memory-efficient and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Let's load the first batch of our data (image and label) and display it using the `matplotlib` library.\n",
    "\n",
    "Recall that the shape returned by\n",
    "```python\n",
    "next(iter(train_loader))\n",
    "```\n",
    "is 32 by 3 by 32 by 32. This shape represents the batch size, number of channels, height, and width of the image, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_batch_images, next_batch_labels = next(iter(train_loader))\n",
    "_first_img = next_batch_images[0] # retrieve the first image from the batch of 32\n",
    "_first_label = next_batch_labels[0] # retrieve the first label from the batch of 32\n",
    "plt.imshow(_first_img.permute(1, 2, 0)) # imshow requires the image to be in height x width x channels format\n",
    "plt.show()\n",
    "print(\"Label: \", CLASSES[_first_label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfw",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Why is the first image different from when we used the dataset directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pHFh",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Model Checkpoints\n",
    "\n",
    "**Checkpoints** are snapshots of the model's state (e.g. model weights) during training that allow you to:\n",
    "\n",
    "- Resume training from any point if interrupted\n",
    "- Compare performance across different epochs\n",
    "- Save the best performing model automatically\n",
    "- Experiment with different training strategies\n",
    "\n",
    "#### What Gets Saved in a Checkpoint?\n",
    "\n",
    "In particular, in this example, the checkpoint will contain:\n",
    "\n",
    "- **Model weights** (`model.state_dict()`) - The learned parameters\n",
    "- **Optimizer state** (`optimizer.state_dict()`) - Learning rates, momentum, etc.\n",
    "- **Epoch number** - Which training epoch this represents\n",
    "- **Loss and accuracy** - Performance metrics at this point\n",
    "\n",
    "#### Why Save Optimizer State?\n",
    "\n",
    "The optimizer state is crucial because modern optimizers like **Adam** maintain:\n",
    "\n",
    "- **Adaptive learning rates** per parameter\n",
    "- **Momentum** from previous gradient updates\n",
    "- **Internal counters** for learning rate scheduling\n",
    "\n",
    "Without saving optimizer state, resuming training would:\n",
    "\n",
    "- Reset learning rates to initial values\n",
    "- Lose accumulated momentum\n",
    "- Potentially cause training instability or slower convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, accuracy, checkpoint_dir='checkpoints_fcnn'):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "    # Save latest checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'fcnn_epoch_{epoch:03d}.pth')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    \"\"\"Load model checkpoint\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    accuracy = checkpoint['accuracy']\n",
    "\n",
    "    print(f\"Loaded checkpoint from epoch {epoch}\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return epoch, loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from mofresh import refresh_matplotlib, ImageRefreshWidget\n",
    "import polars as pl\n",
    "\n",
    "widget = ImageRefreshWidget(src=\"\")\n",
    "\n",
    "@refresh_matplotlib\n",
    "def losschart(data):\n",
    "    df = pl.DataFrame(data)\n",
    "    plt.plot(df[\"epoch\"], df[\"train_loss\"])\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "\n",
    "widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "datalogs = []\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Train the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# To load from a checkpoint, uncomment the line below:\n",
    "# start_epoch, _, _ = load_checkpoint(model, optimizer, 'checkpoints_fcnn/fcnn_epoch_005.pth', device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    running_correct, running_total = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    train_loader_with_progress = tqdm(iterable=train_loader, ncols=120, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    for batch_number, (inputs, labels) in enumerate(train_loader_with_progress):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # predicted = torch.argmax(outputs.data)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log data for tracking\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        running_total += labels.size(0)\n",
    "        running_loss += loss.item()  \n",
    "\n",
    "        if (batch_number % 100 == 99):\n",
    "            train_loader_with_progress.set_postfix({'avg accuracy': f'{running_correct/running_total:.3f}', 'avg loss': f'{running_loss/(batch_number+1):.4f}'})\n",
    "\n",
    "            datalogs.append({\n",
    "                \"epoch\": epoch + batch_number / len(train_loader), \n",
    "                \"train_loss\": running_loss / (batch_number + 1),\n",
    "                \"train_accuracy\": running_correct/running_total,\n",
    "            })\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * running_correct / running_total\n",
    "\n",
    "    datalogs.append({\n",
    "        \"epoch\": epoch + 1, \n",
    "        \"train_loss\": epoch_loss,\n",
    "        \"train_accuracy\": running_correct/running_total,\n",
    "    })\n",
    "\n",
    "    # Save checkpoint every epoch\n",
    "    checkpoint_path = save_checkpoint(\n",
    "        model, optimizer, epoch + 1, epoch_loss, epoch_accuracy\n",
    "    )\n",
    "\n",
    "    # Save best model\n",
    "    if epoch_accuracy > best_accuracy:\n",
    "        best_accuracy = epoch_accuracy\n",
    "        best_path = save_checkpoint(\n",
    "            model, optimizer, epoch + 1, epoch_loss, epoch_accuracy, \n",
    "            checkpoint_dir='checkpoints_fcnn/best'\n",
    "        )\n",
    "        print(f\"New best model saved! Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss={epoch_loss:.4f}, Accuracy={epoch_accuracy:.2f}%\")\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    widget.src = losschart(datalogs)\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for test_images, test_labels in test_loader:\n",
    "        test_images = test_images.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "        test_outputs = model(test_images)\n",
    "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "        test_total += test_labels.size(0)\n",
    "        test_correct += (test_predicted == test_labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Accuracy of the network on the 10000 test images: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    fr\"\"\"\n",
    "The model has an **accuracy of {test_accuracy:.2f}%** on the test set, which is **better than random guessing** (10 classes).  \n",
    "\n",
    "However, this accuracy is **low** compared to **state-of-the-art models**.  \n",
    "\n",
    "The **simple model** we built has **limited capacity** to learn the **complex patterns** in the CIFAR-10 dataset.  \n",
    "\n",
    "Next, we will build a **more advanced model** using convolutional neural network (CNN) to **improve accuracy** and **learn more complex patterns** in the data.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlCL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We end with seeing how the trained model classified 10 random images from a test batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": [
    "_images, _labels = next(iter(test_loader))\n",
    "_images = _images.to(device)\n",
    "_labels = _labels.to(device)\n",
    "_outputs = model(_images).to(device)\n",
    "_, preds = torch.max(_outputs, 1)\n",
    "preds_single = CLASSES[preds.cpu().numpy()]\n",
    "actual_single = CLASSES[_labels.cpu().numpy()]\n",
    "n_to_show = 10\n",
    "indices = np.random.choice(range(len(_images)), n_to_show)\n",
    "\n",
    "_fig = plt.figure(figsize=(15, 3))\n",
    "_fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for _i, idx in enumerate(indices):\n",
    "    img = _images[idx].cpu().numpy().transpose((1, 2, 0))\n",
    "    ax = _fig.add_subplot(1, n_to_show, _i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        -0.35,\n",
    "        \"pred = \" + str(preds_single[idx]),\n",
    "        fontsize=10,\n",
    "        ha=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        -0.7,\n",
    "        \"act = \" + str(actual_single[idx]),\n",
    "        fontsize=10,\n",
    "        ha=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rEll",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Loading Checkpoints\n",
    "\n",
    "The `load_checkpoint()` function restores both the model and optimizer to their exact state from a saved checkpoint.\n",
    "\n",
    "#### Function Usage:\n",
    "```python\n",
    "epoch, loss, accuracy = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "```\n",
    "\n",
    "#### Parameters:\n",
    "- **`model`**: The neural network to load weights into\n",
    "- **`optimizer`**: The optimizer to restore state for\n",
    "- **`checkpoint_path`**: Path to the saved checkpoint file\n",
    "- **`device`**: Device to load the checkpoint on ('cpu', 'cuda', 'mps')\n",
    "\n",
    "#### Returns:\n",
    "- **`epoch`**: The epoch number when this checkpoint was saved\n",
    "- **`loss`**: The training loss at that epoch\n",
    "- **`accuracy`**: The training accuracy at that epoch\n",
    "\n",
    "#### Common Use Cases:\n",
    "\n",
    "**1. Resume Training After Interruption:**\n",
    "```python\n",
    "# Load latest checkpoint and continue from where you left off\n",
    "epoch, _, _ = load_checkpoint(model, optimizer, 'checkpoints_fcnn/fcnn_epoch_025.pth', device)\n",
    "\n",
    "# Continue training from epoch + 1\n",
    "for new_epoch in range(epoch, EPOCHS):\n",
    "    # training code...\n",
    "```\n",
    "\n",
    "**2. Load Best Model for Inference:**\n",
    "```python\n",
    "# Load the best performing model (no need for optimizer state in inference)\n",
    "load_checkpoint(model, optimizer, 'checkpoints_fcnn/best/fcnn_epoch_015.pth', device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "# Use model for predictions...\n",
    "```\n",
    "\n",
    "**3. Compare Different Epochs:**\n",
    "```python\n",
    "# Test epoch 5 performance\n",
    "load_checkpoint(model, optimizer, 'checkpoints_fcnn/fcnn_epoch_005.pth', device)\n",
    "test_model(model)\n",
    "\n",
    "# Test epoch 10 performance\n",
    "load_checkpoint(model, optimizer, 'checkpoints_fcnn/fcnn_epoch_010.pth', device)\n",
    "test_model(model)\n",
    "```\n",
    "\n",
    "#### Important Notes:\n",
    "- The model architecture must match the saved checkpoint\n",
    "- Device mapping handles loading checkpoints across different devices\n",
    "- For inference only, you can pass a dummy optimizer (but it's still required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGlV",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a specific checkpoint and test it\n",
    "# Uncomment the lines below to load and test a checkpoint\n",
    "\n",
    "# # Create a fresh optimizer for loading\n",
    "# _test_optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "# _epoch, _loss, _accuracy = load_checkpoint(model, _test_optimizer, 'checkpoints_fcnn/fcnn_epoch_005.pth', device)\n",
    "\n",
    "# # Test the loaded model\n",
    "# model.eval()\n",
    "# _test_images, _test_labels = next(iter(test_loader))\n",
    "# _test_images = _test_images.to(device)\n",
    "# _test_labels = _test_labels.to(device)\n",
    "# _test_outputs = model(_test_images)\n",
    "# _, _test_preds = torch.max(_test_outputs, 1)\n",
    "\n",
    "# print(f\"Loaded model from epoch {_epoch}\")\n",
    "# print(f\"Sample predictions: {CLASSES[_test_preds[:5].cpu().numpy()]}\")\n",
    "# print(f\"Actual labels: {CLASSES[_test_labels[:5].cpu().numpy()]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
