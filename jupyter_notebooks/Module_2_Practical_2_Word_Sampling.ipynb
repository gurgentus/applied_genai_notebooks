{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Module 2: Practice 2 - Word Sampling\n",
    "\n",
    "In the slides, we explored how the **likelihood of a particular phrase** in a text can be represented using **probabilities**. This concept allows us to **generate new text** that is stylistically and contextually similar to the original.\n",
    "\n",
    "To achieve the most realistic generation, the **probability of the next word** should ideally depend on **all of the preceding text.** However, calculating this is **computationally infeasible** for large documents.\n",
    "\n",
    "To simplify this, we make an assumption: the probability of the next word **depends only on a fixed number of preceding words**, known as the **context.** Recall that, more generally, text is usually broken up into elements called **tokens** (in modern language models these contain only parts of the word), but to keep things relatively simple we will initially work with one-word tokens.\n",
    "\n",
    "In the simplest case, we assume the next word depends only on the **immediately preceding word.** This is an example of a **Markov Assumption** and uses **bigram probabilities** to **generate random text** starting from a given word by sequentially sampling the next word based on its bigram probability.\n",
    "\n",
    "We will also **visualize the bigram probabilities** using a **matrix format**, where each row represents the current word and each column shows the probability of the next word.\n",
    "\n",
    "It will form a **baseline NLP model** that we will **deploy** as part of this module's class activity.\n",
    "As we learn more advanced techniques, we'll **update the model** to improve its performance and capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Bigram Probabilities\n",
    "\n",
    "How should the bigram probabilities be modeled? Intuitively, we can just derive the probabilities from the frequency of the bigram occurance in our data simply as:\n",
    "\n",
    "$$p(w_2 | w_1) = \\frac{C(w_1, w_2)}{C(w_1)}$$\n",
    "\n",
    "where $C(w_1, w_2)$ is the count of the bigram $(w_1, w_2)$ and $C(w_1)$ is the count of the unigram $w_1$.\n",
    "\n",
    "This formula can actually be derived mathematically from the general principle of maximum likelihood estimation (MLE). MLE is a method of estimating the parameters of a statistical model by maximizing the likelihood function, which measures how well the model explains the observed data.\n",
    "\n",
    "Probability of a word sequence $w_1, w_2, \\ldots, w_n$ can be expressed as:\n",
    "\n",
    "$$\n",
    "p(w_1, w_2, \\ldots, w_n) = p(w_1) p(w_2 | w_1) p(w_3 | w_1, w_2) \\cdots p(w_n | w_1, w_2, \\ldots, w_{n-1})\n",
    "$$\n",
    "\n",
    "Using the Markov bigram assumption, this simplifies to:\n",
    "\n",
    "$$\n",
    "p(w_1, w_2, \\ldots, w_n) = p(w_1) p(w_2 | w_1) p(w_3 | w_2) \\cdots p(w_n | w_{n-1})\n",
    "$$\n",
    "\n",
    "An MLE estimator for the bigram probabilities corresponds to **modeling** the probability distribution $p$ in such a way that if $w_1, w_2, \\ldots, w_n$ are the observed words in the training data, the probability above is maximized.\n",
    "\n",
    "Indeed, it can be shown that the MLE estimator for the bigram probabilities is given by the bigram count formula above, which is repeated below for convenience:\n",
    "\n",
    "$$p(w_2 | w_1) = \\frac{C(w_1, w_2)}{C(w_1)}.$$\n",
    "\n",
    "This formula is used in the `analyze_bigrams` function below. The mathematics behind the derivation is beyond the scope of this module, but for those interested here is a nice writeup: https://leimao.github.io/blog/Maximum-Likelihood-Estimation-Ngram/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "\n",
    "def simple_tokenizer(text, frequency_threshold=5):\n",
    "    \"\"\"Simple tokenizer that splits text into words.\"\"\"\n",
    "    # Convert to lowercase and extract words using regex\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    if not frequency_threshold:\n",
    "        return tokens\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(tokens)\n",
    "    # Define a threshold for less frequent words (e.g., words appearing fewer than 5 times)\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens if word_counts[token] >= frequency_threshold\n",
    "    ]\n",
    "    return filtered_tokens\n",
    "\n",
    "def analyze_bigrams(text, frequency_threshold=None):\n",
    "    \"\"\"Analyze text to compute bigram probabilities.\"\"\"\n",
    "    words = simple_tokenizer(text, frequency_threshold)\n",
    "    bigrams = list(zip(words[:-1], words[1:]))  # Create bigrams\n",
    "\n",
    "    # Count bigram and unigram frequencies\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    unigram_counts = Counter(words)\n",
    "\n",
    "    # Compute bigram probabilities\n",
    "    bigram_probs = defaultdict(dict)\n",
    "    for (word1, word2), count in bigram_counts.items():\n",
    "        bigram_probs[word1][word2] = count / unigram_counts[word1]\n",
    "\n",
    "    return list(unigram_counts.keys()), bigram_probs\n",
    "\n",
    "def generate_text(bigram_probs, start_word, num_words=20):\n",
    "    \"\"\"Generate text based on bigram probabilities.\"\"\"\n",
    "    current_word = start_word.lower()\n",
    "    generated_words = [current_word]\n",
    "\n",
    "    for _ in range(num_words - 1):\n",
    "        next_words = bigram_probs.get(current_word)\n",
    "        if not next_words:  # If no bigrams for the current word, stop generating\n",
    "            break\n",
    "\n",
    "        # Choose the next word based on probabilities\n",
    "        next_word = random.choices(\n",
    "            list(next_words.keys()), weights=next_words.values()\n",
    "        )[0]\n",
    "        generated_words.append(next_word)\n",
    "        current_word = next_word  # Move to the next word\n",
    "\n",
    "    return \" \".join(generated_words)\n",
    "\n",
    "def print_bigram_probs_matrix_python(vocab, bigram_probs):\n",
    "    \"\"\"\n",
    "    Print bigram probabilities in a matrix format for Python console output.\n",
    "\n",
    "    Args:\n",
    "    - bigram_probs (dict): A dictionary of bigram probabilities.\n",
    "    \"\"\"\n",
    "    # Print the header row\n",
    "    print(f\"{'':<15}\", end=\"\")\n",
    "    for word in vocab:\n",
    "        print(f\"{word:<15}\", end=\"\")\n",
    "    print(\"\\n\" + \"-\" * (15 * (len(vocab) + 1)))\n",
    "\n",
    "    # Print each row with probabilities\n",
    "    for word1 in vocab:\n",
    "        print(f\"{word1:<15}\", end=\"\")\n",
    "        for word2 in vocab:\n",
    "            prob = bigram_probs.get(word1, {}).get(word2, 0)\n",
    "            print(f\"{prob:<15.2f}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "# Example input text\n",
    "input_text = \"\"\"\n",
    "Darkness cannot drive out darkness, only light can do that\n",
    "\"\"\"\n",
    "# Analyze the bigrams\n",
    "vocab, bigram_probabilities = analyze_bigrams(input_text)\n",
    "print_bigram_probs_matrix_python(vocab, bigram_probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Here we train the model on a short quote by Martin Luther King Jr.\n",
    "\n",
    "Since the training text is very short and each word appears only once or twice, the model has **limited continuation options**.\n",
    "\n",
    "Most words in the bigrams we constructed have only **one possible continuation**, so the model assigns **full probability** to that continuation.\n",
    "\n",
    "For example, since there are only **two bigrams** starting with the word *'darkness'*, the model gives **equal probability** to both possible continuations.\n",
    "\n",
    "Let's generate text starting from this word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_word = \"darkness\"\n",
    "generated_text = generate_text(bigram_probabilities, start_word, num_words=20)\n",
    "\n",
    "print(\"Generated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "As we can see, **most of the generated text** mirrors the training text because **only one continuation** is possible for most words.\n",
    "\n",
    "The only variation occurs after the word **'darkness'**, where the model has **two possible continuations**. Since the selection is **random**, this will get different generated text if we run the generation enough times.\n",
    "\n",
    "To get more interesting and varied text, let's **calculate the bigram probabilities** from a **larger piece of text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get 'Count of Monte Cristo' text from Project Gutenberg\n",
    "book_url = \"https://www.gutenberg.org/cache/epub/1184/pg1184.txt\"\n",
    "\n",
    "# Download the book\n",
    "response = requests.get(book_url)\n",
    "book_text = response.text\n",
    "\n",
    "# Remove Gutenberg header and footer\n",
    "start_marker = \"*** START OF THIS PROJECT GUTENBERG EBOOK\"\n",
    "end_marker = \"*** END OF THIS PROJECT GUTENBERG EBOOK\"\n",
    "\n",
    "start_idx = book_text.find(start_marker)\n",
    "end_idx = book_text.find(end_marker)\n",
    "\n",
    "if start_idx != -1 and end_idx != -1:\n",
    "    book_text = book_text[start_idx + len(start_marker) : end_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "While the generated text is somewhat nonsensical, it is more **unique** and isn't too far from sounding like English. It is, however, limited by the **quality of the bigram probabilities.**\n",
    "\n",
    "We can improve the text by using more advanced techniques, such as:\n",
    "- **Trigram probabilities**: Taking into account the previous two words instead of just one.\n",
    "- **Smoothing techniques**: Handling unseen word combinations more effectively.\n",
    "\n",
    "You will learn about these techniques in a **Natural Language Processing** course. However, they require **more complex models** and **larger datasets** to train on.\n",
    "\n",
    "A major limitation of these models is that they treat **different words as completely separate** entities, even if the words have **similar or identical meanings.**\n",
    "\n",
    "To overcome this, we need a **lower-dimensional latent space** that captures the **meaning of words** and their **relationships.**\n",
    "\n",
    "This is where **word embeddings** come into play:\n",
    "\n",
    "- **Word embeddings** are **dense vector representations** of words that capture **semantic relationships** between words.\n",
    "- These embeddings can be **learned from large text corpora** using **neural networks**, enabling models to understand similarities between words in a **contextual and meaningful** way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "This activity provided an **intuition** for **sampling from a probability distribution** of possible word continuations. By understanding how probabilities influence text generation, we gained insight into the **basic mechanics** behind language models.\n",
    "\n",
    "Starting in the **next module**, we will build **more powerful models** using **neural networks** to generate these probabilities. These advanced methods will allow us to capture **richer contextual relationships**, improve **coherence**, and enhance the **quality of generated text**."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}